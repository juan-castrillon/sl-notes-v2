var relearn_search_index=[{breadcrumb:"",content:`Welcome! Welcome to my personal collection of notes and summaries on tech related topics! this is just a big cheat sheet where I store all the things that I learn in courses, certifications, on classes and on jobs in order for me to have easy access from anywhere.
Here you will find a wealth of information on various tech subjects, including programming languages, DevOps Tools, cloud, software engineering, and more. I have organized my notes by topic and subject, making it easy for you (but most probably me) to find the information needed.
If you are someone that is not me, I hope you find my notes to be helpful and informative.
Thank you for visiting my site, and happy learning!
`,description:"",tags:null,title:"Notes",uri:"/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Scheduling",content:`As part of the scheduling process, the kube-scheduler updates the nodeName field on pods. This behavior can be emulated manually to “skip” the scheduler and manually assign a pod to a node
E.g The pod below will always go to node1
apiVersion: v1 kind: Pod metadata: name: nginx spec: nodeName: node1 containers: - name: nginx image: nginxUpdating running pods Once a pod has been assigned a node, the nodeName field cannot be edited. To change the node, a Binding object is needed. This object (its json representation) must be sent to the pod’s binding API to update the node. An example command to do this might be:
curl --header "Content-Type: application/json" --request POST --data @binding.json http://$SERVER/api/v1/namespaces/default/pods/nginx/bindingWhere $SERVER is the address of the kube-api-server.
`,description:"",tags:null,title:"Manual Scheduling",uri:"/k8s/scheduling/manual/"},{breadcrumb:"Notes \u003e Linux \u003e Using a Linux System",content:`Package managers The process of installing software on Linux can vary depending on the distribution you are using. Here is a general overview of the common methods used to install software on different Linux distributions:
Debian and Ubuntu: Software is typically installed using the apt package manager. Low level manager is normally dpkg Red Hat and Fedora: Software is typically installed using the yum package manager. Low level manager is normally rpm SUSE Family: Software is typically installed using the zypper package manager. Low level manager is normally rpm Other distributions: Some other Linux distributions, such as Slackware, use their own package management systems. You can refer to the distribution’s documentation for information on how to install software. Each package contains the files and other instructions needed to make one software component work well and cooperate with the other components that comprise the entire system.
All linux distributions have a low level and a high level package manager:
The low level, is in charge of installing single packages correctly. The high level usually calls the low level one and additionally, is in charge of resolving dependencies. Other ways of installing It’s also worth noting that there are other ways to install software on Linux, such as downloading binaries, compiling from source code, using flatpak, or using snap which is increasingly popular, but package managers are the most common and recommended method.
Common operations Operation RH Family Debian Family Install package rpm -i foo.rpm dpkg --install foo.deb Install package, dependencies yum install foo apt-get install foo or apt install foo Remove package rpm -e foo.rpm dpkg --remove foo.deb Remove package, dependencies yum remove foo apt-get remove foo or apt remove foo Remove package, deps, and config apt-get remove --purge foo or apt purge foo Remove “left-behind” packages yum autoremove apt-get autoremove or apt autoremove Update package rpm -U foo.rpm dpkg --install foo.deb Update package, dependencies yum update foo apt-get install foo Update entire system yum update apt-get dist-upgrade Show all installed packages rpm -qa or yum list installed dpkg --list or apt list --installed Get information on package rpm -qil foo dpkg --listfiles foo Show packages named foo yum list "foo" apt-cache search foo or apt search foo Show all available packages yum list apt-cache dumpavail foo What package is file part of? rpm -qf file dpkg --search file Debian-based distributions APT and APT-GET apt and apt-get are both package management tools for Debian and Ubuntu-based Linux distributions but apt is a more user-friendly tool, abstracting some of the functionality from other commands. apt-get is considered to be more powerful and flexible, and mostly used in automation for the -y option for example.
Repositories apt (or apt-get) stores a list of repositories or software channels in the file /etc/apt/sources.list and in any file with the suffix .list under the directory /etc/apt/sources.list.d/
A software repository is a collection of software packages that can be installed on the system. These packages are organized and managed by Ubuntu developers and are easily accessible to users via the apt package manager.
By editing these files from the command line, we can add, remove, or temporarily disable software repositories.
Format A typical line on the file looks like this:
deb http://ch.archive.ubuntu.com/ubuntu/ saucy main restricted deb: These repositories contain binaries or precompiled packages. These repositories are required for most users. deb-src: These repositories contain the source code of the packages. Useful for developers. http://archive.ubuntu.com/ubuntu: The URI (Uniform Resource Identifier), in this case a location on the internet. saucy is the release name or version of the distribution (found with lsb_release -sc). main \u0026 restricted are the section names or components. There can be several section names, separated by spaces. Adding new repositories Adding a new line with the above format will add a new repository. To “load” them is important to retrieve the updated package list with
sudo apt-get update PPA Repositories Adding Launchpad PPA (Personal Package Archive) is possible conveniently via the command add-apt-repository in Ubuntu or addrepo on Debian. It looks like: sudo add-apt-repository ppa:\u003crepository-name\u003e
`,description:"",tags:null,title:"Installing Software",uri:"/linux/using-a-linux-system/installing-software/"},{breadcrumb:"Notes \u003e Linux \u003e System",content:`The boot process In linux, the boot process occurs when initiating the system. It goes from the moment the computer is powered on until the UI is fully operational and ready.
This is a sequential Process
1. BIOS Basic Input Output System Initializes the hardware (including monitor and keyboard) Tests the main memory (Power On Self Test) Normally installed in a ROM in the motherboard 2. Boot Loader First Stage: In Old systems: The BIOS looks for executable boot code in a certain part of the disk (Master Boot Record (MBR)). This code examines the partition table and finds a bootable partition. Also finds a second-stage bootloader (e.g. GRUB) and loads in into RAM In New Systems (UEFI): UEFI Firmware reads boot entries (Information about where is which os, where in the EFI partition is the bootloader). It also finds the bootloader and starts it. Second Stage: Programs like GRUB allow to choose the OS. Once decided, it loads the kernel and the initial RAM disks (in RAM) and passes control to the kernel. (Kernel first job is to decompress itself) Second stage bootloaders are located in /boot Some second-stage bootloaders are GRUB (for GRand Unified Boot loader), ISOLINUX (for booting from removable media), and DAS U-Boot (for booting on embedded devices/appliances) 3. Kernel initialization and initial RAM Disk initramfs When the kernel is loaded in RAM, it initializes and configures the computer’s memory and also configures all the hardware attached to the system. The filesystem contains programs and binary files that perform all actions needed to mount the proper root filesystem After the root filesystem has been found, it is checked for errors and mounted. At boot time, the boot loader loads the kernel and the initramfs image into memory and starts the kernel. The kernel checks for the presence of the initramfs and, if found, mounts it as / and runs /sbin/init. 4. /sbin/init It handles the mounting and pivoting over to the final real root filesystem. Is the parent process of nearly all processes Set ups processes and networking, mounts filesystems, etc. Keeps the system running (Managing other processes, cleaning up after them, etc) Shut down cleanly Most distributions have a link from /sbin/init to systemd
SystemD Targets (former Runlevels) When initializing the system, systemd defines a set of services that need to run so that the system is considered ready. However, this is not a fix set and it depends on the “mode” one wants to initialize the system in.
The following “modes” (or targets in systemd) are common:
multi-user.target: Start the system into non GUI (terminal) input graphical.target: Start the system into GUI input Targets provide a way to declare service and other target dependencies. There a several targets defined in the systemd documentation
To see the default target in a system:
systemctl get-defaultTO change it:
systemctl set-default multi-user.target`,description:"",tags:null,title:"System Startup",uri:"/linux/system/system-startup/"},{breadcrumb:"Notes \u003e Linux",content:`The Linux kernel is the core of the operating system. A full Linux distribution consists of the kernel plus a number of other software tools for file-related operations, user management, and software package management
Distribution Families There are more than 200 distributions. The most commonly used can be categorized in three families:
Red Hat Family Systems (including CentOS and Fedora) SUSE Family Systems (including openSUSE) Debian Family Systems (including Ubuntu and Linux Mint). Red Hat Some of the key facts about the Red Hat distribution family are:
Fedora serves as an upstream testing platform for RHEL. CentOS is a close clone of RHEL, while Oracle Linux is mostly a copy with some changes (in fact, CentOS has been part of Red Hat since 2014). CentOS 8 has reached end of life while CentOS 7 will do it in 2024 A heavily patched version 3.10 kernel is used in RHEL/CentOS 7, while version 4.18 is used in RHEL/CentOS 8. It supports hardware platforms such as Intel x86, Arm, Itanium, PowerPC, and IBM System z. It uses the yum and dnf RPM-based yum package managers to install, update, and remove packages in the system. RHEL is widely used by enterprises which host their own systems. SUSE Some of the key facts about the SUSE family are listed below:
SUSE Linux Enterprise Server (SLES) is upstream for openSUSE. Kernel version 4.12 is used in openSUSE Leap 15. It uses the RPM-based zypper package manager to install, update, and remove packages in the system. It includes the YaST (Yet Another Setup Tool) application for system administration purposes. SLES is widely used in retail and many other sectors. Debian Some key facts about the Debian family are listed below:
The Debian family is upstream for Ubuntu, and Ubuntu is upstream for Linux Mint and others. Kernel version 4.15 is used in Ubuntu 18.04 LTS. It uses the DPKG-based APT package manager (using apt, apt-get, apt-cache, etc.) to install, update, and remove packages in the system. Ubuntu has been widely used for cloud deployments. While Ubuntu is built on top of Debian and is GNOME-based under the hood, it differs visually from the interface on standard Debian, as well as other distributions. `,description:"",tags:null,title:"Distributions",uri:"/linux/distributions/"},{breadcrumb:"Notes \u003e Linux",content:`Linux is a powerful and flexible operating system that is widely used in a variety of settings, from personal computers to servers and supercomputers. One of the strengths of Linux is the wealth of documentation and help available to users. Below are several options to get documentation or help about commands and the system in general:
man pages Short for manual pages, the man program is in charge of searching and showing documentation.
A given topic may have multiple pages associated with it and there is a default order determining which one is displayed when no options or section number is specified (in /etc/man_db.conf) The man pages are divided into chapters numbered 1 through 9 man command #Shows the manual entry for the command man -f command #Lists all entries (can be more than one) for the command whatis command #Same as above man -k command #List all entries where the command is mentioned (can be from other commands) apropos command #Same as above man -a command #display all "command" pages in all chapters, one after the otherTLDR As an alternative for the man pages, the the tldr-pages project provides summarized documentation for commands, with a focus on practical examples. It does not come installed by default, but after installing it, here is an example when running tldr tldr :
foo@bar:~$ tldr tldr tldr Display simple help pages for command-line tools from the tldr-pages project.More information: https://tldr.sh. - Print the tldr page for a specific command (hint: this is how you got here!): tldr {{command}} - Print the tldr page for a specific subcommand: tldr {{command}}-{{subcommand}} - Print the tldr page for a command for a specific [p]latform: tldr -p {{android|linux|osx|sunos|windows}} {{command}} - [u]pdate the local cache of tldr pages: tldr -u GNU Info Similar to man but works with subsections and links Accessible through info tool. To navigate:
Action Key Move Arrows Next Page Page Up Previous Page Page Down Select menu item Enter Quit q Help h Go to next n Go to previous p Move one node up in the index u --help and help --help :Most commands provide a short way to get some reference passing the --help flag. help Is a program that can be used inside bash shells to get help about commands that run especially built-in bash versions. A list of these commands is shown using help Other sources Include:
Desktop help system: All Linux desktop systems have a graphical help application. This contains desktop specific help as well as some rendered man and info pages. Can be ran from GUI or CLI (For example, for GNOME gnome-help or yelp) Package documentation: Linux documentation is also available as part of the package management system. Usually, this documentation is directly pulled from the upstream source code, but it can also contain information about how the distribution packaged and set up the software. Normally available in /usr/share/doc Online resources: Include forums, and doc pages. Also ebooks, like this one `,description:"",tags:null,title:"Documentation and Help",uri:"/linux/documentation-and-help/"},{breadcrumb:"Notes \u003e Linux \u003e System",content:"",description:"",tags:null,title:"Files",uri:"/linux/system/files/"},{breadcrumb:"Notes",content:`Linux is an open source operating system (OS). An operating system is the software that directly manages a system’s hardware and resources, like CPU, memory, and storage. The OS sits between applications and hardware and makes the connections between all of your software and the physical resources that do the work.
In particular Linux provides the kernel, on top of which all different OS (Distributions) are built. The linux kernel acts like a “glue” between software and hardware, allowing the applications to talk to the underlying physical devices.
Files are stored in a hierarchical filesystem, with the top node of the system being the root or simply “/”. Whenever possible, Linux makes its components available via files or objects that look like files. Processes, devices, and network sockets are all represented by file-like objects and can often be worked with using the same utilities used for regular files.
Linux is a fully multitasking (i.e., multiple threads of execution are performed simultaneously), multiuser operating system with built-in networking and service processes known as daemons in the UNIX world.
Info Linux was inspired by UNIX, but it is not UNIX.
`,description:"",tags:null,title:"Linux",uri:"/linux/"},{breadcrumb:"Notes \u003e Linux",content:"",description:"",tags:null,title:"System",uri:"/linux/system/"},{breadcrumb:"Notes \u003e Linux \u003e System \u003e Files",content:`Linux distinguishes 3 main categories of files:
Regular files Directories Special files The command:
file \u003cfile\u003ecan be used to determine the type of a file
Special files Under “special”, linux understands all files that are not regular. They are:
Character files These files, usually located in the /dev directory, represent devices that interact serially with the operating system (e.g., a mouse or keyboard).
Block files These are also normally located in /dev but represent block devices (devices that read and write data in fixed-size blocks, such as hard disks and RAM )
Links Links provide a way to relate two or more files in linux:
Hard Links: A hard link associates multiple filenames with the same block of data Deleting the last hard link results in the removal of that data. Otherwise,the link “keeps it” Soft links: A symlink acts like a shortcut, pointing to another file Deleting a symlink does not affect its target file. Sockets Sockets are special files that facilitate communication between different processes.
Named Pipes Also known as FIFOs, these files allow one process to deliver its output directly to another using a unidirectional data flow.
`,description:"",tags:null,title:"Types",uri:"/linux/system/files/types/"},{breadcrumb:"Notes \u003e Linux \u003e System",content:`Linux interacts with attached hardware mounting it as device file (See file types)
Device creation process When a new device is attached, the corresponding device driver in the Linux kernel identifies the state change and generates a uEvent. This event is forwarded to the user-space device manager daemon, udev, which then dynamically creates a device file under the /dev filesystem for the new device.
Accessing devices information Accessing logs The kernel logs can be inspected for logs about device attachment, errors, etc. This can be done for example with
dmsgUDEV information One can query udev about a certain device:
udevadm info --query=path --name=/dev/sda5A “live monitor” can also be put in place to see real time udev signals and kernel messages
udevadm monitorPCI device information One can query specifically the PCI devices attached. Normally these are all devices that are not storage like mouse, ethernet card, wifi card, keyboard, etc:
lspciBlock device information In the same way, block devices (normally storage) can be listed:
lsblkGeneral information Finally detailed information on the hardware configuration of the machine is also available. This can be a bit too much but can report exact memory configuration, firmware version, mainboard configuration, CPU version and speed, cache configuration, among others:
lshw`,description:"",tags:null,title:"Devices",uri:"/linux/system/devices/"},{breadcrumb:"Notes \u003e Linux \u003e System",content:`The linux kernel is the main component in all linux distributions. It serves as the core interface between computer hardware and application processes, allowing the applications to talk to the underlying physical devices.
Modules The kernel is a monolithic (handling most tasks on itself) software, but it is extensible using modules
The kernel is responsible for many critical tasks like:
Memory Management It keeps track of the memory used/free and which process are using/needing memory Divides the memory into two spaces Kernel space: Processes that the kernel starts, and are needed for the system to run, these have unrestricted access to resources User space: All other processes, access to resources is controlled and limited to certain extend. Process Management Control which processes can access resources (e.g CPU) and for how long Device drivers Act as “interpreter” between applications and underlying hardware System calls Processes communicate with the kernel via functions (system calls) that the kernel processes and executes (e.g open() to open a file) title To check the existing kernel version running in a system the following command can be used:
uname -r `,description:"",tags:null,title:"Kernel",uri:"/linux/system/kernel/"},{breadcrumb:"Notes \u003e Linux \u003e Using a Linux System \u003e Shell",content:`The shell can be customized in all possible ways to facilitate work and save time.
Prompt Customization (Bash) To setup a custom prompt in bash, the environment variable PS1 is used. One can temporarily set it up to other values to try out, but to make it persistent the variable needs to be editted in .bashrc
Tip Alternatively, tools like this can be used
Documentation about available options can be found here
`,description:"",tags:null,title:"Customization",uri:"/linux/using-a-linux-system/shell/customization/"},{breadcrumb:"Notes \u003e Linux \u003e Using a Linux System",content:`The shell in linux is a program that allows the user to interact with the underlying OS by sending text commands.
Popular shells include:
Bourne Again Shell (bash) zsh sh Current shell can be determined reading the environment variable $SHELL. Furthermore, using
chshAllows one to change the shell for the user
Assumed shell In these docs, it is assumed the shell is bash
Command types Command come in two types. The command type can be used with a command as argument to verify which type a certain command is
Internal: Come built in with the shell. E.g. echo External: Are binaries installed in the system. E.g git, mv Environment Variables Environment variables are user-definable values that affect how running processes behave on the system. The OS sets up a group of variables by default. At any time, the set up environment variables for a user can be seen with
printenvTo create environment variables, there is two options
export FOO=bar: This will make the variable extend to all processes created from this shell FOO=bar: The variable will not go further `,description:"",tags:null,title:"Shell",uri:"/linux/using-a-linux-system/shell/"},{breadcrumb:"Notes \u003e Linux",content:"",description:"",tags:null,title:"Using a Linux System",uri:"/linux/using-a-linux-system/"},{breadcrumb:"Notes",content:`Kubernetes in a container orchestration tool that allows easy configuration, deployment and management of high volume containerized workload and services.
It was developed by Google,first as an internal product and then made public as part of the Linux Foundation.
Why Orchestration? When dealing with containers in production, normally deployment conditions and dependencies are complex. These can include:
Multiple instances required Dynamic scaling required (More use, more instances) Complex inter-container interaction required Inter-container dependencies (One needs another one to work) Resource Management Load distribution Reliability (if one container goes down another should come up) Multiple hosts Orchestration solutions tackle this and many other scenarios and abstract container management. These include Kubernetes, but also things like Nomad, Mesos or Docker Swarm.
`,description:"",tags:null,title:"Kubernetes",uri:"/k8s/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Tools",content:`Kustomize is a tool that allows for managing and customizing Kubernetes resource configuration files.
It is an extra layer that allows to simplify configuration by avoiding repeating code, and using the concepts of base and overlays to generate custom configurations while leaving the original YAML untouched and usable as is.
Usage Kustomize works by defining a kustomization.yaml file. This file defines mainly two things:
Which resources (k8 configs) should be managed by kustomize Which customizations need to be made Once this is defined, one can call
kustomize build dir/ Which will print the modified configurations. This can be piped to apply in the cluster directly
kustomize build dir/ | kubectl apply -f - Alternatively, kubectl ships with kustomize built in (normally older version), and can be called with kubectl apply -k dir/ to call customize and then deploy the result
Directory Structure Kustomize can be used in many different ways. It can be used to manage a single folder of resources (e.g adding a common tag to all resources)
- dir | folder - kustomization.yaml | file-alt | red - deployment.yaml | file-alt | blue - service.yaml | file-alt | blueIn here the kustomization file will look like:
apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - deployment.yaml - service.yaml # Customizations to be appliedFor more real scenarios, configuration can be split into directories, with the root kustomization.yaml including each directory.
- dir | folder - kustomization.yaml | file-alt | red - api | folder - kustomization.yaml | file-alt | red - deployment.yaml | file-alt | blue - service.yaml | file-alt | blue - db | folder - kustomization.yaml | file-alt | red - deployment.yaml | file-alt | blue - service.yaml | file-alt | blueIn here the root kustomization file will look like below, and the directory ones, like the one above:
apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - api/ - db/ # Customizations to be appliedFinally, Kustomize provides an organized ways of dealing with different enviroments with a base-overlays setup:
- base | folder - kustomization.yaml | file-alt | red - api | folder - kustomization.yaml | file-alt | red - deployment.yaml | file-alt | blue - service.yaml | file-alt | blue - db | folder - kustomization.yaml | file-alt | red - deployment.yaml | file-alt | blue - service.yaml | file-alt | blue - overlay | folder - dev | folder - kustomization.yaml | file-alt | red - api | folder - kustomization.yaml | file-alt | red - deployment.yaml | file-alt | blue - service.yaml | file-alt | blue - db | folder - kustomization.yaml | file-alt | red - deployment.yaml | file-alt | blue - service.yaml | file-alt | blue - prod | folder - kustomization.yaml | file-alt | red - api | folder - kustomization.yaml | file-alt | red - deployment.yaml | file-alt | blue - service.yaml | file-alt | blue - db | folder - kustomization.yaml | file-alt | red - deployment.yaml | file-alt | blue - service.yaml | file-alt | blueIn here the kustomization files will look like:
base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - api/ - db/prod(dev)/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../base/ - api/ - db/ # Customizations to be appliedCustomizing configurations Transformers Transformers allow modification of all resources. These include things like:
Attach labels Add annotations Add name prefix/suffix Add a namespace These actions are done with common transformers:
commonLabels: org: example # All resources under this k.yaml will have this label commonAnnotations: branch: master # All resources under this k.yaml will have this annotation namePrefix: pre- # All resources under this k.yaml will have the name pre-\u003cname\u003e nameSuffix: -dev # All resources under this k.yaml will have the name \u003cname\u003e-dev namespace: lab # All resources under this k.yaml will have the namespace definedSimilarly, images transformers can be used to modify container images in all resources (that have one). Either the complete image can be replaced:
images: - name: nginx newName: haproxy newTag: 2.4or just the tag
images: - name: nginx newTag: 2.4Patches Patches also allow for customization, but enable a more “surgical” approach than transformers. This means that certain (not all) resources can be targeted.
Patches can be defined in-line or as separate files. They can also be specified either in:
JSON 6902 format, defined by three values: Operation, Target (The patch target selects resources by group, version, kind, name, namespace, labelSelector and annotationSelector. ), Value (except if operation is remove) Using SMP (Strategic Merge Patch): Uses K8s configurations, which copy the target but only shows the fields that need change. I find JSON6902 easier to read, so all my examples will be using that format
Available operations are:
# add: creates a new entry with a given value - op: add path: /some/new/path value: value # replace: replaces the value of the node with the new specified value - op: replace path: /some/existing/path value: new value # copy: copies the value specified in from to the destination path - op: copy from: /some/existing/path path: /some/path # move: moves the node specified in from to the destination path - op: move from: /some/existing/path path: /some/existing/destination/path # remove: delete's the node('s subtree) - op: remove path: /some/path # test: check if the specified node has the specified value, if the value differs it will throw an error - op: test path: /some/path value: "my-node-value"Examples apiVersion: v1 kind: Pod metadata: name: simple-webapp-color labels: hello: oe spec: containers: - name: simple-webapp-color image: simple-webapp-color apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization patches: - target: kind: Pod name: simple-webapp-color # Instead of inline, path can be used to point to a patch file patch: |- - op: add path: /metadata/labels/goodbye value: bye - target: kind: Pod name: simple-webapp-color # Instead of inline, path can be used to point to a patch file patch: |- - op: replace path: /metadata/labels/hello value: hi apiVersion: v1 kind: Pod metadata: name: simple-webapp-color labels: hello: hi goodbye: bye spec: containers: - name: simple-webapp-color image: simple-webapp-color apiVersion: v1 kind: Pod metadata: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color - name: logger image: logger apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization patches: - target: kind: Pod name: simple-webapp-color patch: |- - op: remove path: /spec/containers/1 - target: kind: Pod name: simple-webapp-color patch: |- - op: replace path: /spec/containers/0/image value: redis - target: kind: Pod name: simple-webapp-color # /- appends to the list patch: |- - op: add path: /spec/containers/- value: name: lb image: nginx apiVersion: v1 kind: Pod metadata: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: redis - name: lb image: nginx Components Kustomize enables the use of “reusable” blocks of configuration (resources + patches) to avoid copying code even further. These blocks are called components.
They are very useful for customizations that only certain overlays should have, and that can be turned on on demand (e.g features).
For example:
For this an extra folder is created:
- k8s/ | folder - base/ | folder - kustomization.yaml | file-alt | red - api-depl.yaml | file-alt | blue - components/ | folder - caching/ | folder - kustomization.yaml | file-alt | red - deployment-patch.yaml | file-alt | green - redis-depl.yaml | file-alt | blue - db/ | folder - kustomization.yaml | file-alt | red - deployment-patch.yaml | file-alt | green - postgres-depl.yaml | file-alt | blue - overlays/ | folder - dev/ | folder - kustomization.yaml | file-alt | red - premium/ | folder - kustomization.yaml | file-alt | red - standalone/ | folder - kustomization.yaml | file-alt | red Warning Component kustomization.yaml files define Component objects instead of Kustomization. These objects have no resources. More info here
As the necessary deployments and patches are defined in components, the environment files can just call the components to include them:
overlays/premium/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../../base/ components: - ../../components/ # Other Customizations to be applied`,description:"",tags:null,title:"Kustomize",uri:"/k8s/tools/kustomize/"},{breadcrumb:"Notes \u003e Kubernetes",content:"",description:"",tags:null,title:"Tools",uri:"/k8s/tools/"},{breadcrumb:"Notes \u003e Kubernetes",content:"",description:"",tags:null,title:"App Configuration",uri:"/k8s/appconfig/"},{breadcrumb:"Notes \u003e Kubernetes \u003e App Configuration",content:`It is clear from the spec.containers being an array, that a pod can be defined with multiple containers inside. Two or more containers that are in the same pod share:
Lifecycle: Created and terminated together Network space: All containers share a network space, and can find each other using localhost Storage volumes: ALl containers have access to the same volumes which can be ued to transfer files between them This configuration is common to enable patterns like sidecar containers (e.g logging, nw management).
Warning When two or more containers are part of a pod, they are expected to live equally. That means that if any of the container fails or exits, the pod is terminated (or restarted). For different lifecycles see below
Init Containers K8s introduces the concept of init containers for containers that do not live equally but must be in the same pod. In this sense, an init container is one that performs a task and finishes before the main container(s) start.
When defining multiple init containers they run in sequential order according to their definition. When they are all finished, the main one(s) start.
apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox:1.28 command: \\['sh', '-c', 'echo The app is running! \u0026\u0026 sleep 3600'\\] initContainers: - name: init-myservice image: busybox command: \\['sh', '-c', 'git clone ;'\\]`,description:"",tags:null,title:"Multi-Container Pods",uri:"/k8s/appconfig/multi/"},{breadcrumb:"Notes \u003e Kubernetes \u003e App Configuration",content:`As most applications tend to follow the 12 factor config standard, setting environment variables becomes important to configure the applications. In K8s,there is 3 ways of setting them
Environment Variables in pod spec As part of the pod definition, one can set environment variables that will be made available inside the container (similar to using the -e flag in docker):
apiVersion: v1 kind: Pod metadata: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color ports: - containerPort: 80 env: - name: APP_COLOR value: blue - name: APP_MODE value: prodConfigMap ConfigMap objects can be mounted as environment variables for containers. There are two options:
Import all values from the ConfigMap apiVersion: v1 kind: Pod metadata: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color ports: - containerPort: 80 envFrom: - configMapRef: name: myconfigmap Import certain values apiVersion: v1 kind: Pod metadata: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color ports: - containerPort: 80 env: - name: APP_COLOR valueFrom: configMapRef: name: myconfigmap key: colorSecrets Secret objects can be mounted as environment variables for containers. There are two options:
Import all values from the Secret apiVersion: v1 kind: Pod metadata: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color ports: - containerPort: 80 envFrom: - secretRef: name: mysecret Import certain values apiVersion: v1 kind: Pod metadata: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color ports: - containerPort: 80 env: - name: APP_COLOR valueFrom: secretKeyRef: name: my-secret key: color`,description:"",tags:null,title:"Environment Variables",uri:"/k8s/appconfig/env/"},{breadcrumb:"Notes \u003e Kubernetes \u003e App Configuration",content:`As most applications run as containers when deployed in a K8s cluster, one can define custom commands and arguments to run the container.
In this sense, a parallel can be drawn to configuring a container in docker:
In Docker In K8s config What it does CMD args Command that will be executed when the container starts or alternatively arguments for the ENTRYPOINT of the container ENTRYPOINT command Specifies the command to run when the container starts (completely replacing Dockerfile ENTRYPOINT). For example
apiVersion: v1 kind: Pod metadata: name: nginx-pod labels: app: nginx spec: containers: - name: nginx-container image: nginx command: - sleep # Override default ENTRYPOINT args: - "5" # Pass 5 as the arg for sleep`,description:"",tags:null,title:"Commands and arguments",uri:"/k8s/appconfig/commands/"},{breadcrumb:"Notes",content:`Ansible is a configuration management tool whose main purpose is to help automatically provision servers
In a standard architecture, a machine (called an ansible control host) serves as a middle man between the operator of ansible (me) and the servers to provision. This control host connects directly to the servers (via SSH) and issues commands to provision them.
An alternative to this, is to have the operator workstation (my laptop), act as a control host directly, skipping the machine in the middle.
Its important to highlight that in both of these setups, the servers to be provisioned don’t have Ansible installed
`,description:"",tags:null,title:"Ansible",uri:"/ansible/"},{breadcrumb:"Notes \u003e Ansible",content:`Copying a file For this the copy module can be used For example: - name: Send custom html page copy: src: default_site.html # files directory is assumed dest: /var/www/html/index.html owner: root group: root mode: 0644Unzipping a file unarchive module can be used It supports downloading a file directly - name: Install terraform unarchive: # Needs unzip installed src: https://releases.hashicorp.com/terraform/0.12.28|terraform_0.12.28_linux_amd64.zip dest: /usr/local/bin remote_src: yes mode: 0755 owner: root group: rootChange lines in config files The lineinfile module allows to match a file and replace it in the target server It needs to be tested before hand, as any typo can lead to duplicate lines in the file leading to misconfiguration - name: change e-mail address for admin lineinfile: path: /etc/httpd/conf/httpd.conf regexp: '^ServerAdmin' line: ServerAdmin somebody@somewhere.net when: ansible_distribution == "CentOS" register: httpdManage Services Services can be managed, started, restarted, enabled, etc. from ansible using the service module This is a generic module that acts acts as a proxy to the underlying service manager module (like systemd) so not all options are available (similar to package module and apt or dnf) - name: Start and enable httpd in a CentOS machine service: name: httpd state: started enabled: true - name: change e-mail address for admin lineinfile: path: /etc/httpd/conf/httpd.conf regexp: '^ServerAdmin' line: ServerAdmin somebody@somewhere.net when: ansible_distribution == "CentOS" register: httpd - name: restart httpd (CentOS) only if the config file was changed service: name: httpd state: restarted when: httpd.changed User management Several modules in ansible allow admins to do user management on the servers like builtin.user or posix.authorized_key A very common use case is to have a bootstrap playbook to provision a user which then ansible can use for other playbooks (by having passwordless sudo for example) - hosts: all become: true tasks: - name: create simone user user: name: simone groups: root - name: add ssh key for simone authorized_key: user: simone key: "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAe7/ofWLNBq3+fRn3UmgAizdicLs9vcS4Oj8VSOD1S/ ansible" - name: add sudoers file for simone copy: content: 'simone ALL=(ALL) NOPASSWD: ALL' dest: /etc/sudoers.d/simone owner: root group: root mode: 0440`,description:"",tags:null,title:"Cookbook",uri:"/ansible/cookbook/"},{breadcrumb:"Notes \u003e Ansible",content:"",description:"",tags:null,title:"Execution",uri:"/ansible/execution/"},{breadcrumb:"Notes \u003e Ansible \u003e Execution",content:`Ansible templates are files used to dynamically generate configuration files or scripts. These templates are written using the Jinja2 templating language, which allows to include variables, loops, conditionals, and more.
Templates are typically stored in the templates directory of an Ansible role or playbook, and they are processed by the template module in Ansible. When you use this module, you specify the source template file and the destination path on the target machine. During execution, Ansible replaces any variables and expressions in the template with their corresponding values.
Example of Using a Template in Ansible Here’s an example of a simple template file (config.j2):
server_name = {{ inventory_hostname }} port = {{ http_port }}To deploy this template, use the following task in your playbook:
- name: Deploy configuration file template: src: config.j2 dest: /etc/myapp/config.confThis task would create a file /etc/myapp/config.conf on the target machine with the variables replaced by their actual values from the playbook or inventory.
`,description:"",tags:null,title:"Templates",uri:"/ansible/execution/templates/"},{breadcrumb:"Notes \u003e Ansible \u003e Execution",content:`There is a “special” kind of tasks called handlers, which only run when triggered by changes in another tasks.
This behavior substitutes the pattern of combining register with when: var changed. It allows for more flexibility and easier management when multiple tasks can trigger the same handler
In roles, there is a handlers directory to place them, but they can also be added to a playbook.
They are triggered by notify and their name, which must be static (more info here)
Multiple tasks can trigger a handler. Notifying the same handler multiple times will result in executing the handler only once regardless of how many tasks notify it.
In the same way, a task can have multiple notify. It is important to note here that handlers are executed in the order they are defined in the handlers section, not in the order listed in the notify statement.
Example:
tasks: - name: Template configuration file ansible.builtin.template: src: template.j2 dest: /etc/foo.conf notify: - Restart memcached handlers: - name: Restart memcached ansible.builtin.service: name: memcached state: restarted`,description:"",tags:null,title:"Handlers",uri:"/ansible/execution/handlers/"},{breadcrumb:"Notes \u003e Ansible \u003e Execution",content:` Roles are a way to organize ansible tasks (not plays) in order to allow them to be reused and easily shared They provide a predefined structure that allows to group tasks with variables, templates, files, etc. Similar to modules in terraform In the playbooks they can be called like: - hosts: all become: true roles: - myroleRoles can also be used in a task with import_role. When running with roles ansible will execute the roles after any pre_task but before any other task in the play. When calling it from a task with import_role the order is respected.
Most importantly, import_roles is dynamic, which allows to do things like have variables for the role name, use loops to execute them, skip them.
--- - hosts: webservers tasks: - name: Print a message ansible.builtin.debug: msg: "this task runs before the example role" - name: Include the example role include_role: name: example - name: Print a message ansible.builtin.debug: msg: "this task runs after the example role" Tip If not using the roles directory, a path can be passed instead of just the name
Defined Structure For each role, ansible expect a folder inside the roles directory named as the role (e.g roles/myrole)
Inside each role folder, the following structure is standard:
tasks to put the taskbooks in, with the default one always being main.yml defaults to put low precedence default values for role variables in, with the default one always being main.yml handlers to put the handlers that will be imported in the parent play in, with the default one always being main.yml vars to put high precedence variables provided by the role with the default one always being main.yml files to put any files referenced in the role tasks (like in copy) templates to put any role templates metadata metadata for the role, including role dependencies and optional Galaxy metadata such as platforms supported. This is required for uploading into galaxy as a standalone role, but not for using the role in your play. With the default one always being main.yml The command ansible-galaxy init role_name can be used to initialize this folder directory quickly
Sharing roles Roles can be downloaded and shared via ansible galaxy, which is an online open-source, public repository of Ansible content Community roles can be installed and then called from playbooks `,description:"",tags:null,title:"Roles",uri:"/ansible/execution/roles/"},{breadcrumb:"Notes \u003e Ansible \u003e Execution",content:`Referenced in the playbook as {{ name }} variables allow for consolidation of playbooks as well as flexibility.
Variables can be defined:
In the inventory file. Particular values per host or group can be defined and will be picked up [mygroup] 1.2.3.4 apache_package=apache2 [mygroup:vars] other_var=something In playbooks themselves using vars block or vars_file for external files
In a host_vars directory. In this standard directory, each file corresponds to a host (either IP or hostname) and ansible will pick up automatically the variables defined there for each host. Similarly a group_vars directory can be used where the name of the files are the name of the groups in the inventory. More info here
`,description:"",tags:null,title:"Variables",uri:"/ansible/execution/variables/"},{breadcrumb:"Notes \u003e Ansible \u003e Execution \u003e Playbooks",content:`when keyword Conditional logic can be introduced into Ansible playbooks using the when keyword. This allows to define a condition to execute a task. Any variable, including the ones resulted from gather_facts, can be used for the condition. This means, that if a task should only run on Ubuntu servers, the task could look like this:
- name: install apache2 package # Name of the task apt: # Using the builtin apt module name: apache2 # Name of the package to install update_cache: true # Before installing run apt update when: ansible_distribution == "Ubuntu" gather_facts can be called as an ad-hoc task ansible -m gather_facts to see the different variables and values available
hosts and groups Another way to conditionally execute a playbook is to group the servers (based on characteristics, function, etc) and then using the hosts entry in the plays to target the different groups
--- - name: Update packages hosts: all # Running on all nodes become: true tasks: - name: install updates (centos) dnf: update_only: yes update_cache: yes when: ansible_distribution == "CentOS" - name: Provision web servers hosts: web_servers become: true tasks: - name: install apache2 package and php support (centos) dnf: name: - httpd - php state: latest when: ansible_distribution == "CentOS" - name: Provision dbs hosts: db_servers become: true tasks: - name: install mariadb (centos) dnf: name: mariadb state: latestWith an inventory that looks like
[web_servers] 1.2.3.4 [db_servers] 5.6.7.8.9Tags To focus (or skip) certain tasks from the playbook explicitly, tags can be used. This metadata allows ansible to just run or skip certain tasks. This helps to target particular architecture, quickly debug changes, etc.
Tags are defined in the different tasks as a comma separated string
- name: Update packages tags: always hosts: all # Running on all nodes become: true tasks: - name: install updates (centos) dnf: update_only: yes update_cache: yes when: ansible_ditribution == "CentOS" - name: Provision web servers tags: web_servers,centos hosts: web_servers become: true tasks: - name: install apache2 package and php support (centos) dnf: name: - httpd - php state: latest when: ansible_ditribution == "CentOS"Using the cli, ansible-playbook --list-tags playbook.yml can be used to see available tags. At the same time ansible-playbook --tags centos can be used to run only tasks that have a tag, with the opposite being ansible-playbook --skip-tags centos
The special keywords always and never can be used in the tags to guarantee that no matter which tags are passed, a task always (or never) runs.
`,description:"",tags:null,title:"Conditional Tasks",uri:"/ansible/execution/playbooks/conditional/"},{breadcrumb:"Notes \u003e Ansible \u003e Execution",content:`Ansible Playbooks offer a repeatable, reusable, simple configuration management and multi-machine deployment system. They are files that allow to define a set of instructions to execute to reach a desired state. Sames as the [ad-hoc]({\u003c ref “ansible/execution/adhoc” \u003e}}) commands, they make use of modules to execute things.
They are written in YAML format and run with the ansible-playbook binary
ansible-playbook -K my_plabook.ymlAn example of a simple playbook can be seen below:
- name: Install apache # Name of the whole playbook hosts: all # Running on all nodes become: true # Same as --become, run this as root tasks: - name: install apache2 package # Name of the task apt: # Using the builtin apt module name: apache2 # Name of the package to install update_cache: true # Before installing run apt updateMore modules, and options can be added to achieve complex behaviors.
Running a playbook When running a playbook, ansible first always runs a gather_facts task to gain information on the servers. After running, the results will be summarized in the PLAY RECAP, for example:
ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 This gives an overview of what the command achieved and wheter or not it succeded
ok shows the tasks that did not fail (always 1 plus because of the gather_facts tasks) changed shows the tasks that actually changed something in the server (Running an install task twice will only change once) unreachable will report if the server could not be reached while running the task skipped will make sense for conditional tasks (that run depending if something is true) `,description:"",tags:null,title:"Playbooks",uri:"/ansible/execution/playbooks/"},{breadcrumb:"Notes \u003e Ansible \u003e Execution",content:`Although the core of ansible is using playbooks, the cli tool can be used to directly execute commands or run modules. They work great for tasks that are run not that often or tests.
For running a command the following is needed:
Inventory group name Key to use when connecting via SSH Inventory file location Module or command This can all be passed in the cli. For example, the following command uses the ping module to verify connection to all servers:
ansible all --key-file ansible.key -i inventory.ini -m pingMost of the repetitive values however, can be put inside a ansible.cfg file. A local file will override configuration in /etc/ansible/ansible.cfg. Configuration can also be done via environment variables:
[defaults] inventory = inventory.ini private_key_file = ./ansible.key remote_user = ansibleThat way, the following command (that uses the gather_facts module to get information of the servers) works:
ansible all -m gather_factsElevated commands Ansible uses existing privilege escalation systems to execute tasks with root privileges or with another user’s permissions. Because this feature allows one to ‘become’ another user, different from the user that logged into the machine (remote user), its called become. The become keyword uses existing privilege escalation tools like sudo, su, etc (using sudo by default)
If nothing else is specified, the become keyword will try to run the commands as the root user. The password can be passed via a file (using the --become-password-file option) or be prompted (-K or --ask-become-pass). More information can be seen in the docs
Example of ad-hoc elevated commands are:
Update package cache in all servers (apt update)
ansible all -m apt -a update_cache=true --become --ask-become-pass Update and upgrade all packages (apt upgrade)
ansible all -m apt -a upgrade=yes --become --ask-become-pass `,description:"",tags:null,title:"Ad-hoc commands",uri:"/ansible/execution/adhoc/"},{breadcrumb:"Notes \u003e Ansible \u003e Execution",content:`To know which machines are available to control, ansible needs an inventory. This simply describes (with the ip addresses) which machines are to be controlled.
The simplest inventory is a single file with a list of hosts and groups. The default location for this file is /etc/ansible/hosts. However, using the -i \u003cpath or expression\u003e option(s) or using the configuration system the file can be changed.
Grouping The inventory files allow also to group and set up different configuration values for each server. They can be defined in ini or yaml formats (depending what is easier to read given the architecture)
Even if no groups are defined in the inventory file, Ansible creates two default groups: all and ungrouped. The all group contains every host. The ungrouped group contains all hosts that don’t have another group aside from all. Threfore every host will always belong to at least 2 groups (all and ungrouped or all and some other group)
Once the inventory is organized, commands can be run on all host in a group with:
ansible mygroup ... ansible all ...More information can be found in the documentation
Dynamic inventories Inventory plugins can be utilized to create dynamic inventories, based on cloud providers API or other resources. This can be used in combination with static inventories to reflect complex setups
`,description:"",tags:null,title:"Inventory",uri:"/ansible/execution/inventory/"},{breadcrumb:"Notes \u003e Ansible \u003e Setup",content:`Ansible can be installed from their direct repository, using pip or directy from many linux distribution package stores.
More info can be seen here
To install in ubuntu:
sudo apt update sudo apt install software-properties-common sudo add-apt-repository --yes --update ppa:ansible/ansible sudo apt install ansible`,description:"",tags:null,title:"Installation",uri:"/ansible/setup/installation/"},{breadcrumb:"Notes \u003e Ansible",content:"",description:"",tags:null,title:"Setup",uri:"/ansible/setup/"},{breadcrumb:"Notes \u003e Ansible \u003e Setup",content:`SSH needs to be configured, as it is the main way that ansible uses to send commands to the servers. This means that:
The control node must have openssh client installed The servers must have openssh server installed As general good practice, the use of ssh keys instead of passwords is recommended. There is multiple type of keys to generate, the example below generates a ED25519 key:
ssh-keygen -t ed25519 -C "some key"The key can have a password to unlock it. This is recommended for personal keys, but in the case of creating a key for ansible to use, it’s better to skip it.
Once a key is created it can be added to a server using the command:
ssh-copy-id -i ~/.ssh/id_rsa.pub \u003cUSER\u003e@\u003cIP\u003eFor this the user must already exist in the machine, and the machine running the command should also have ssh access.
Order of execution The first time that a connection to a new server is established, the ssh client will ask for confirmation. It is common to do this first connection manually to avoid “messing” with Ansible when it tries to connect. Alternatively, ansible can also be configured to automatically accept
`,description:"",tags:null,title:"SSH",uri:"/ansible/setup/ssh/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Scheduling",content:`As shown in process, part of the scheduling depends on the defined priority of the pods to be scheduled. To control this, k8s allows for the definition of objects called PriorityClass which determined how critical a pod is.
The range of priorities that can be defined, can go from 10^9 to -2^31 (-2,147,483,648). Priority classes are also used for system components deployed (thing of the kube-system namespace), and these ones go from 10^9 to 2*10^9 (and maybe more).
Once defined, priority classes are global to the cluster (e.g not attached to namespaces). By default, the two priorities used for system components exist (system-cluster-critical, system-node-critical)
Very importantly, the classes allow to define how to behave with respect to lower priority pods. The options are: Preempt them (PreemptLowerPriority) or not (never) with the default being Preemption.
Tip By default, all pods that do not defined a priority class, have priority 0, this can be changed assigning a priority class as default with globalDefault
Definition To define it, the following YAML structure is used
apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: my-prio value: 1000000000 description: "Ultra high prio tasks" preemptionPolicy: PreemptLowerPriority globalDefault: Once defined, pods can be created referencing a class:
apiVersion: v1 kind: Pod metadata: name: mypod spec: priorityClassName: my-prio`,description:"",tags:null,title:"Priority",uri:"/k8s/scheduling/prio/"},{breadcrumb:"Notes \u003e Kubernetes",content:"",description:"",tags:null,title:"Scheduling",uri:"/k8s/scheduling/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Monitoring",content:`Access application logs Using the kubectl tool, one can access container logs directly (similar to docker logs) :
kubectl logs pod kubectl logs -f pod #Follow logs kubectl logs pod container #For multi container pods`,description:"",tags:null,title:"Apps",uri:"/k8s/monitoring/apps/"},{breadcrumb:"Notes \u003e Kubernetes",content:"",description:"",tags:null,title:"Monitoring",uri:"/k8s/monitoring/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Monitoring",content:`K8s does not offer monitoring out of the box for its components. Multiple solutions like Prometheus, datadog, etc can be integrated.
metrics-server Altough not a part of the cluster by default, K8s provides to option to create a limited resource metrics pipeline/
This pipeline provides a limited set of metrics related to cluster components such as the HorizontalPodAutoscaler controller, as well as the kubectl top utility. These metrics are collected by the lightweight, short-term, in-memory metrics-server.
metrics-server discovers all nodes on the cluster and queries each node’s kubelet for CPU and memory usage. The agent then pulls the information either from the underlying OS or using something like cAdvisor to pull pod (container specific metrics)
The metrics-server is a set of K8s components (Deployment, Service, etc) that can be directly installed with:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yamlBy then, usage statistics can be seen with
kubectl top node kubectl top pod Attention Metrics Server is meant only for autoscaling purposes. For example, don’t use it to forward metrics to monitoring solutions, or as a source of monitoring solution metrics. In such cases one can collect metrics from Kubelet /metrics/resource endpoint directly.
`,description:"",tags:null,title:"Cluster Components",uri:"/k8s/monitoring/cluster/"},{breadcrumb:"Notes \u003e Kubernetes",content:"",description:"",tags:null,title:"AccessControl",uri:"/k8s/accesscontrol/"},{breadcrumb:"Notes \u003e Kubernetes \u003e AccessControl",content:`Admission controllers are pieces of code that run inside the kube-api-server and process requests to modify (create, edit, etc) resources after they have been authenticated and authorized, but before they go to scheduling.
They can be used both to verify configuration (Validating controllers) or take action (Mutating controllers). Some examples include:
Deny a request that aims to create a pod in non-existent namespace (NamespaceLifecycle controller, type validating) If a request aims to create a pod in a non-existent namespace, first create the namespace (NamespaceAutoProvision controller, mutating type) Order of execution Mutating controllers always run first, so that their result can be validated.
By default, the kube-api-server includes a long list of controllers, although not all are enabled. They can be enabled/disabled using the configuration options in the kube-api-server service or container.
Enabling/Disabling Controllers To enable controllers (or see which controllers are currently enabled), the option --enable-admission-plugins of the kube-api-server binary is used. In systems where this runs as a service, this can be checked in the service definition (using something like systemctl cat). In kubeadm systems, the server runs as a static pod, so the argument is passed in the pod definition present in the static pod path (See static pods)
Similarly, the option --disable-admission-plugins is used to disable the plugins.
Custom controllers (Webhooks) K8s provides two webhooks (controllers) (MutatingAdmissionWebhook and ValidatingAdmissionWebhook) that allow to plug in custom controller implementations.
The webhooks send a JSON request to a configured endpoint where the custom code listens. It then expects a standard response that uses to approve or deny the request like any other controller.
Custom controllers are normally deployed as containers in the cluster (Deployment objects) with a Service object in front. Once they are up and listening, a WebhookConfiguration object is created to pass the service name to the admission controller. Below is an example:
apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: name: "pod-policy.example.com" webhooks: - name: "pod-policy.example.com" clientConfig: service: namespace: "webhook-namespace" name: "webhook-service" # Name of the deployed service caBundle: "Ci0tLS0tQk......tLS0K" rules: # What ype of requests use the custom webhook - apiGroups: [""] apiVersions: ["v1"] operations: ["CREATE"] resources: ["pods"] scope: "Namespaced"Below, the image shows the complete admission process, including custom controllers:
`,description:"",tags:null,title:"Admission Controllers",uri:"/k8s/accesscontrol/admission/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Scheduling",content:`The default kube-scheduler works in the following way:
When a pod is submitted to the scheduler, it enters a scheduling queue along with other pending pods. Filter Phase: Nodes that cannot meet the pod’s resource requirements (e.g., nodes lacking 10 CPUs) are filtered out. Scoring Phase: Remaining nodes are scored based on resource availability after reserving the required CPU. For example, a node with 6 CPUs left scores higher than one with only 2. Binding Phase: The pod is assigned to the node with the highest score. In each of the phases, logic and decision making is implemented by plugins
Scheduling Plugins The image below shows different plugins that work in different stages of the scheduling pipeline:
For example:
Priority Sort Plugin: Sorts pods in the scheduling queue according to their priority. Node Resources Fit Plugin: Filters out nodes that do not have the needed resources. Node Name Plugin: Checks for a specific node name in the pod specification and filters nodes accordingly. Node Unschedulable Plugin: Excludes nodes marked as unschedulable. Scoring Plugins: During the scoring phase, plugins (such as the Node Resources Fit and Image Locality plugins) assess each node’s suitability. They assign scores rather than outright rejecting nodes. Default Binder Plugin: Finalizes the scheduling process by binding the pod to the selected node. A complete list of default plugins can be found here
Going deeper, K8s scheduling framework, provides “extension points” to which plugins “plug”. This is the entry point for default plugins, but can also be used by custom written plugins to implement missing functionality. These extension points are defined in different stages of the pipeline :
More info about the particular extension points can be seen in the docs
`,description:"",tags:null,title:"Scheduler Process",uri:"/k8s/scheduling/process/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Scheduling",content:`Multiple schedulers If a different scheduling algorithm, or more particular requirements are needed, a custom scheduler can be deployed. Moreover, once can even run multiple schedulers simultaneously alongside the default scheduler and instruct Kubernetes what scheduler to use for each of the pods.
Following good practices, the scheduler can be packaged and distributed as a binary (to create a service) or a container (to create a K8s deployment).
Once deployed, pods can choose which scheduler to use, for example:
apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx schedulerName: my-custom-schedulerTHe process of deploying a custom scheduler is shown here
Scheduling profiles As a more lightweight alternative to multiple schedulers, K8s (Starting in 1.18) now supports the definitions of different profiles using the same scheduler binary.
This approach minimizes operational overhead and prevents potential race conditions that can arise when multiple processes schedule workloads on the same node.
It works by allowing profile definitions in the scheduler configuration definition. These profiles can then manipulate (add or disable) scheduling plugins thus defining different behaviors from one another. Here is an example definition file:
apiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: - schedulerName: my-scheduler-2 plugins: score: disabled: - name: TaintToleration enabled: - name: MyCustomPluginA - schedulerName: my-scheduler-3 plugins: preScore: disabled: - name: '*' score: disabled: - name: '*' - schedulerName: my-scheduler-4 Tip This file is normally mounted as ConfigMap in the scheduler pod or as an argument for the binary (in case of a service)
Pods can then specify the scheduler they use in their definition files as above.
`,description:"",tags:null,title:"Multiple Scheduler and Profiles",uri:"/k8s/scheduling/multiple/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Scheduling",content:`K8s provides a way to “override” the scheduler and deploy pods statically. These static pods are managed directly by the kubelet daemon on a specific node, without the API server observing them.
The kubelet automatically tries to create a mirror Pod on the Kubernetes API server for each static Pod. This means that the Pods running on a node are visible on the API server, but cannot be controlled from there.
Tip The static pod names will be suffixed with the node hostname with a leading hyphen.
Noticeably, static pod deployments are used by the kubeadm tool to deploy control plane components (e.g api server, scheduler, etc.)
Deploying static pods The kubelet read static pod definitions for a defined folder. This can either be passed as an argument --pod-manifest-path or as the staticPodPath part of the configuration file (--config) to the kubelet.service.
Finding the path systemctl cat kubelet.service can be useful to find the path. Most of the times is /etc/kubernetes/manifests
To deploy a static pod, the only required action is to create a pod definition in this folder. The kubelet will pick it up automatically and deploy it. Similarly, for any changes done to the file, the kubelet will reflect them in the deployment.
`,description:"",tags:null,title:"Static Pods",uri:"/k8s/scheduling/static/"},{breadcrumb:"Notes \u003e Kubernetes",content:"",description:"",tags:null,title:"Commands",uri:"/k8s/commands/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Commands",content:"Get daemon sets kubectl get daemonsets",description:"",tags:null,title:"DaemonSet",uri:"/k8s/commands/daemonset/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Scheduling",content:`When scheduling, K8s takes into consideration available resources (CPU and Memory) at the nodes. However, by default, there is no upper limit, so eventually pods could “fill up” nodes. To prevent this, and to ensure certain resources at the container, pod or namespace level, the concept of request and limits is introduced.
Pod (Container) level In each container in a pod, two thing can be defined to specify resource requirements and limits:
requests: These act as requirements They define the guaranteed amount of CPU and memory that a container will get limits Define the maximum amount of CPU and memory that the container will get In case of going over the CPU limit, the CPU available for the container will be throttled If going over the memory limit, the container will be killed (OOM) Units CPU is specified in cores, which i typically equivalent to one vCPU in cloud environments like AWS, GCP, or Azure. In the definitions, fractional values can be be used with prefixes (e.g 100m is equal to 0.1). For memory, either bytes are used, either in base 1000 (K,M,G) or 1024 (Ki, Mi, Gi).
Below is an example of a pod definition with resource allocation:
apiVersion: v1 kind: Pod metadata: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color resources: requests: memory: "1Gi" cpu: 1 limits: memory: "2Gi" cpu: 2Combination Behavior - No Limit Limit Set No Request A container can use as much as it wants, potentially starving others in the same node K8s assumes an implicit request equal to the limits Request Set Guarantees that containers will get their minumum resources needed, but they are free to use more. This is the recommended configuration for CPU, NOT MEMORY. More info The container is guaranteed a certain amount but can peak and use up to the limit, however idle resources might be generated in the case of CPU (reserved but not utilized). Recommended for memory Namespace level `,description:"",tags:null,title:"Resource Limits",uri:"/k8s/scheduling/resources/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Commands",content:"Add labels to a node kubectl label nodes node_name key=value",description:"",tags:null,title:"Nodes",uri:"/k8s/commands/nodes/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Scheduling",content:`As opposed to taints which prevents pods from going to nodes, node labels are used either with selector or affinity definitions to make a pod run in a certain node.
Node Selectors In a simple scenario, node selector definitions (part of the pods YAML) are used to match labels in the nodes. The example below binds the pod with nodes that have the size=Large label
apiVersion: v1 kind: Pod metadata: name: myapp-pod spec: containers: - name: data-processor image: data-processor nodeSelector: size: LargeNode Affinity For more complex scenarios, node affinity definitions are used. These also work based on labels but allow for more complex rules.
It has two types:
requiredDuringSchedulingIgnoredDuringExecution: The scheduler can’t schedule the Pod unless the rule is met. This functions like nodeSelector. preferredDuringSchedulingIgnoredDuringExecution: The scheduler tries to find a node that meets the rule. If a matching node is not available, the scheduler still schedules the Pod somewhere. In here a weight can be defined that is added everytime a soft condition is met in a node. In that sense the highest scoring node might be chosen In its more complex syntax, it also allows more complex matching expressions with an operator field (In, NotIn, Exists, DoesNotExist, Gt and Lt) and array inputs. Here is the same example as above:
apiVersion: v1 kind: Pod metadata: name: myapp-pod spec: containers: - name: data-processor image: data-processor affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: size operator: In values: - Large`,description:"",tags:null,title:"Node Selectors and Affinity",uri:"/k8s/scheduling/selectors/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Scheduling",content:`When scheduling, the concept of taints and tolerations solves the following problem:
How to make nodes only run certain pods?
Attention It’s important to note that taints and tolerations does NOT make pods run in certain nodes. It is a limitation on nodes.
Taints Taints are conditions applied to nodes that describe which pods to reject. They have 3 main components:
Key: Label key that the toleration needs to match Value: Label value that the toleration needs to match Behavior: This tells the scheduler what exactly to do: NoSchedule: If a pod cannot tolerate the taint, it will not be scheduled in the node. If any pod is already running and does not match, it will keep running PreferNoSchedule: If a pod cannot tolerate the taint, the scheduler might try to put it somewhere else, but it might end up here anyways NoExecute: Similar to NoSchedule but non tolerant running pods are evicted Taints are put on a node using kubectl:
kubectl taint nodes \u003cnode\u003e key:value:NoExecuteTolerations Tolerations are set on pods to “overcome” node taints, making the pod “compatible” with certain tainted nodes. This is defined in the pod’s definition:
apiVersion: v1 kind: Pod metadata: name: myapp-pod spec: containers: - name: nginx-container image: nginx tolerations: - key: "app" operator: "Equal" value: "blue" effect: "NoSchedule"A toleration “matches” a taint if:
The keys are the same The effects are the same, and: The operator is Exists (in which case no value should be specified), or the operator is Equal and the values should be equal. `,description:"",tags:null,title:"Taints and Tolerations",uri:"/k8s/scheduling/taints/"},{breadcrumb:"Notes \u003e Kubernetes",content:`In order to categorize and filter resources of all kinds, K8s uses the concept of labels and selectors. In this sense, resources like Pods, Deployment, Nodes, etc. are created with a set of labels that can then be used by a selector to “pick” resources and target the correct ones.
Labels are normally added in the metadata field of objects. For example in a pod :
apiVersion: v1 kind: Pod metadata: name: nginx labels: app: app1 tier: frontend spec: containers: - name: nginx image: nginxSelectors are then used in other resources. For example a replica set
apiVersion: apps/v1 kind: ReplicaSet metadata: name: myapp-rs spec: selector: # How to find which pods to manage? matchLabels: # Using the pod labels app: app1`,description:"",tags:null,title:"Labels and selectors",uri:"/k8s/labels/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Commands",content:"Imperative Create namespace kubectl create namespace dev-ns",description:"",tags:null,title:"Namespaces",uri:"/k8s/commands/namespaces/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Commands",content:"Imperative Create service for an existing pod kubectl expose pod pod_name --port=port --name service_name # By default, ClusterIP kubectl expose pod pod_name --port=port --target-port=tp --name service_name kubectl expose pod pod_name --type=NodePort --port=port --name service_name # Port on node will be random, if this is not ok, it has to be done with a file kubectl create service clusterip service_name --tcp=port:tp # Will not use the labels on a pod as selectors, but will search for labels app=service_nameGet service kubectl get svcCreate service YAML File kubectl expose pod pod_name --port=port --name service_name --dry-run=client -o yaml kubectl create service clusterip service_name --tcp=port:tp --dry-run=client -o yaml",description:"",tags:null,title:"Services",uri:"/k8s/commands/services/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Commands",content:"Get Replica Sets kubectl get replicaset kubectl get rsGet single replica set kubectl describe replicaset rs_nameImperative Scale replica set ad-hoc kubectl scale --replicas=6 replicaset rs_nameImperative Destroy replica set kubectl delete replicaset rs_name",description:"",tags:null,title:"ReplicaSets",uri:"/k8s/commands/replicasets/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Commands",content:'Get all pods kubectl get pods kubectl get pods -o wide # Adds more info like ipGet single pod kubectl describe pod pod_nameGet pods based on selector kubectl get pods --selector key=valueGenerate POD Manifest YAML file kubectl run pod_name --image=img --dry-run=client -o yamlImperative Run pod ad-hoc kubectl run pod_name --image image_name kubectl run pod_name --image=image_name --labels="k1=v1,k2=v2" kubectl run pod_name --image=image_name --port=80 --expose=true # Create a service directlyImperative Destroy pod kubectl delete pod pod_name',description:"",tags:null,title:"Pods",uri:"/k8s/commands/pods/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Commands",content:"Imperative Create Deployment kubectl create deployment --image=img d_name kubectl create deployment --image=img d_name --replicas=4 # Create with +1 replicasGenerate Deployment YAML file kubectl create deployment --image=img --dry-run=client -o yamlImperative Scale deployment kubectl scale deployment d_name --replicas=4See rollout status (following logs) kubectl rollout status deployments/d_nameSee revision history kubectl rollout history deployments/d_nameImperative Rollback to previous revision kubectl rollout undo deployments/d_nameImperative Change deployment image ad-hoc kubectl set image deployments/d_name cont_name=image",description:"",tags:null,title:"Deployments",uri:"/k8s/commands/deployments/"},{breadcrumb:"Notes \u003e Kubernetes \u003e Commands",content:`Imperative vs Declarative management commands In K8s, there is three paradigms to manage general resources:
Imperative approach with commands: Using kubectl commands like run, create, edit, expose, scale to perform operations Imperative approach using a configuration file: Using kubectl commands like create, replace and delete with -f options Declarative approach using a configuration file: Using kubectl apply -f kubectl apply In general, due to best practices like maintainability, repeatability and documentation, the declarative approach (Using kubectl apply is preferred). This works based on 3 files:
Input file Live object configuration file Last applied configuration (field in file above) When deciding what to do, the program
Compares input configuration with last applied Implement any changes in the live object configuration [!CAUTION] Because of the presence of the last_applied_configuration field in the LOCF, objects that were created and managed wit apply must not be changed to be managed with an imperative approach
Imperative Commands Imperative Create resource from YAML kubectl create -f res.yamlImperative Replace resource from YAML kubectl replace -f res.yamlImperative Delete resource from YAML kubectl delete -f res.yamlImperative Edit existing object definition in place kubectl edit object_type object_name # Does not edit a YAML definition if there is one`,description:"",tags:null,title:"General",uri:"/k8s/commands/general/"},{breadcrumb:"Notes \u003e Kubernetes",content:`Cluster Architecture In kubernetes, different machines organize in a cluster. A Kubernetes cluster consists of a control plane (master node) plus a set of worker machines, called nodes, that run containerized applications. Every cluster needs at least one worker node in order to work.
Nodes: This machines run the actual workloads. They have a container runtime that allows to run containers in them. Master: This machines manage the cluster, store information about its workers and monitors them. They are in charge of receiving the workloads and dividing them in the workers, as well as all parts of orchestration (moving the load, replication, etc.) Components K8s is composed of the following components, that reside in either master or node machines:
Master nodes API Server Provides a single interface that allows to control the cluster. CLIs, tools, UIs and users use this to interact with the cluster (like kubectl).
This component sits in the middle of all cluster actions that add, remove or edit something. In this way, a request to create a pod has the following process:
The API server authenticates and validates the request. It Constructs a pod object (initially without a node assignment) and updates the etcd store. Notifies the requester that the pod has been created. The scheduler continuously monitors the API Server for pods that need node assignments. Once a new pod is detected, the scheduler selects an appropriate node and informs the API Server. The API Server then updates the etcd datastore with the new assignment It passes this information to the kubelet on the worker node. The kubelet deploys the pod via the container runtime and later updates the pod status back to the API Server for synchronization with etcd. It handles:
Authentication \u0026 Validation of requests Interaction with etcd (In fact is the only component that access it) Communication with kube-scheduler and kubelet It is installed as a single binary, and normally deployed as a service in the master machines. In here, in the ExecStart field, many arguments can be passed to configure it. K8s installations from kubeadm, install it as a pod in the kube-system namespace.
etcd Distributed Key value store that is used to store cluster information.
To interact with an installation of etcd, the etcdctl CLI is used. For example etcdctl set k1 v1, or etcdctl put k1 v1 in v3.
In K8s, etcd is used to store cluster information that includes configuration data, state and metadata. As a rule of thumb, most kubectl get commands, read information from etcd. Most actions on the cluster, first are implemented, then registered into etcd and just then are deemed done. Normally, data is stored under the registry parent key, following a folder structure (e.g registry/pods/...).
It is installed as a single binary, and normally deployed as a service in the master machines. In here, in the ExecStart field, many arguments can be passed to configure it, including details about clustering. K8s installations from kubeadm, install it as a pod in the kube-system namespace.
Scheduler Component that distributes the work in the different nodes deciding but does not placing the pods in the worker nodes. It takes different factors like resource usage, constraints, among others into consideration. It does this by:
Filtering out nodes Ranking possible nodes and choosing the best It is installed as a single binary, and normally deployed as a service in the master machines. In here, in the ExecStart field, many arguments can be passed to configure it, including details about clustering. K8s installations from kubeadm, install it as a pod in the kube-system namespace.
Controller Manager Brains of the operation. Is in charge of controllers (processes) which monitor containers/nodes and act accordingly to always have a desired state (Similar to a PID controller). Some of the controller include NodeController, ReplicaSets, etc.
It is installed as a single binary, and normally deployed as a service in the master machines. In here, in the ExecStart field, many arguments can be passed to configure it, including details about clustering. K8s installations from kubeadm, install it as a pod in the kube-system namespace.
Worker nodes Container runtime Software used to run containers. Traditionally used docker, but recently supports only those who implement the CRI interface. These include containerd, cri-o among others.
kubelet Agent that runs in all nodes and interacts with the api-server in the masters. It manages and monitors the containers and the nodes. IT also interacts with the container runtime to provision the pods.
It is installed as a single binary, and normally deployed as a service in the master machines. In here, in the ExecStart field, many arguments can be passed to configure it, including details about clustering.
Opposite to other components, K8s installations from kubeadm do not install kubelet in the nodes. It has to be manual.
kube-proxy Lightweight process that runs on every node. Its key function is to monitor for Service creations and configure network rules that redirect traffic to the corresponding pods. One common method it uses is by setting up iptables rules.
It is installed as a single binary, and normally deployed as a service in the master machines. In here, in the ExecStart field, many arguments can be passed to configure it, including details about clustering. K8s installations from kubeadm, install it as a DaemonSet, ensuring it runs in each node.
`,description:"",tags:null,title:"Architecture",uri:"/k8s/arch/"},{breadcrumb:"Notes \u003e Kubernetes",content:`Pods Pods are the smallest deployable unit in K8s. They are a “logical” grouping that encapsulates containers that are to be managed via Kubernetes.
Pods can contain one container (standard use case) or multiple ones that are somehow bundled together (e.g sidecar containers). All containers in the pod share:
Network (can reach each other with localhost) Storage Host (co-located) Schedule (lifecycles are tied to one another) In general, pods are thought to be volatile and replaceable. Each instance of a pod represents an instance of an app that can be scaled up/down at any point.
YAML Definition Like every other K8s element, a pod can be created using kubectl, but the recommended way to do it is using a YAML definition. The same fields are common to all elements: apiVersion, kind, metadata and spec. In the case of a pod, here is a short example
apiVersion: v1 # For pods is always v1 kind: Pod metadata: name: nginx-pod # Name of the pod, only valid if created standalone labels: # KV pairs that can be used to separate pods from each other app: nginx spec: containers: - name: nginx-container image: nginx # Image to pull from docker registry (or private one)ReplicaSet ReplicaSets are kubernetes objects used to maintain a stable set of replicated pods running within a cluster at any given time. Different instances of a pod are called replicas and this objects manage them so that a desired number of them are running at all times (can also be 1 or 0)
Legacy k8s used ReplicationController objects for this purpose. This has been deprecated in favor of ReplicaSet and its use is not recommended
A ReplicaSet has three main features: a matcher to detect which pods to manage, a pod template for creating new pods whenever existing ones fail, and a replica count for maintaining the desired number of replicas that the controller is supposed to keep running. A ReplicaSet also works to ensure additional pods are scaled down or deleted whenever an instance with the same label is created.
Finally, these controllers can be used with existing pods (created before the replica set). In this case, matching pods will be counted towards the replica count.
YAML Definition apiVersion: apps/v1 kind: ReplicaSet metadata: name: myapp-rs labels: app: nginx spec: selector: # How to find which pods to manage? matchLabels: # Using the pod labels app: nginx replicas: 4 # 4 Instances must be running at all times (including existing ones) template: # Under here, a normal YAML definition for a pod is used metadata: name: nginx-pod # Does not matter much, pod names will start with the replica set name labels: app: nginx # Make sure label matches the matcher spec: containers: - name: nginx-container image: nginxDeployment Deployments are kubernetes abstract elements that provides a way to manage updates for Pod and ReplicaSet elements
These elements encapsulate (are a level higher) the other two, and allows to control updates in scenarios like:
New image version Scale up / down Rollout deployments Rollback It creates and manages ReplicaSet elements underneath to manage the pods. When rolling out/back, new ones are created to pass pods following a given strategy.
To apply updates, deployments use the concept of a rollout. These represent a new revision (version) of a deployment in which something changed (see above). Creating or modifying a deployment creates a rollout.
The rollout can follow one of two strategies:
Recreate: Destroy all old pods, and then spin all the new ones. This approach leads to having downtime in this period RollingUpdate: This is the default strategy. Destroy one old pod (or more) and bring up a new one. Repeat. This approach has no downtimes as the application (either in the old or new version) will always be available. For fine tuning, the parameters maxUnavailable and maxSurge (maximum number of Pods that can be created over the desired number of Pods) can be used. YAML Definition Definition is similar to a replica set, but with kind: Deployment
apiVersion: apps/v1 kind: Deployment metadata: name: http-frontend spec: strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 25% maxSurge: 25% replicas: 3 selector: matchLabels: tier: frontend template: metadata: name: http-frontend-pod # Does not matter much, pods will have the name of the deployment as prefix labels: tier: frontend spec: containers: - name: http-frontend-cont image: httpd:2.4-alpineDaemonSet DaemonSet are kubernetes objects, similar to ReplicaSet, that ensures that a pod is run in all (or certain) pods in a cluster. This includes any potential new nodes, that will be given an instance of the pod automatically when joining the cluster.
These type of objects are generally used for:
Monitoring services Networking components (e.g kube-proxy) Other uses Under the hood, these objects make use of Node Affinity to create a pod for each of the nodes and interact with the scheduler to deploy it.
YAML Definition apiVersion: apps/v1 kind: DaemonSet metadata: name: monitoring-daemon spec: selector: matchLabels: app: monitoring-agent #Matches the pod below template: metadata: labels: app: monitoring-agent spec: containers: - name: monitoring-agent image: monitoring-agentServices Services are kubernetes abstractions that help expose groups of Pods over a network. Each Service object defines a logical set of endpoints (usually these endpoints are Pods) along with a policy about how to make those pods accessible.
These (using the underlying plugin-implemented network) allow for communication :
Between pods in the cluster (backend - frontend for example) Using an IP address external to the cluster (IP of the node for example) They enable a loose coupling between pods, which is necessary as pods and their addresses can constantly change.
Depending on the type of “exposure” desired, there are different types of services that can be created
On creation, all service elements are assigned a cluster-wide unique IP (cluster IP) which can be used to refer to it.
NodePort services These type of services, expose a port in a pod (or a series of pods) as a port in the host machine. With this, the address of the host machine (external to the kubernetes cluster) can be used to access the application
Node ports assigned can range from 30000 to 32767. If no port is defined, a random free one in that range will be used
Pods are selected using a selector field in the definition. If a service is created and matches multiple pods, or pods in multiple hosts, K8s automatically includes them under the service, and they will be reachable using the external IP of any of the nodes (even if not running an instance) in the defined port.
YAML Definition apiVersion: v1 kind: Service metadata: name: my-service spec: type: NodePort ports: - protocol: TCP targetPort: 80 # If not provided, assumed to be same as port port: 80 # Port in which app will be available inside the cluster (cluster ip) nodePort: 30008 # If not provided a free port in the range is assigned selector: label1: value1ClusterIP services As seen above, each service gets assigned an internal cluster IP where it will route request from port to targetPod on the pods. This type of service (without linking to external IP) is a ClusterIP service. This is the default type of service in kubernetes and allow inter-pod communication inside the cluster.
Once created, these services provide a single (non volatile) interface that pods can use to find other pods. They can do so by using the cluster IP of the service or its name.
YAML Definition apiVersion: v1 kind: Service metadata: name: back-end spec: type: ClusterIP ports: # No node port is used - targetPort: 80 port: 8080 # This app will be available in \u003cCLUSTER_IP\u003e:8080 and route to port 80 on the pods selector: label1: value1LoadBalancer services This type of service is particular to K8s clusters running in given cloud providers. When defined, K8s will provision a native cloud-provisioner load balancer for the application which can be used to unify the access to a single IP
YAML Definition apiVersion: v1 kind: Service metadata: name: back-end spec: type: LoadBalancer ports: - targetPort: 80 port: 8080 nodePort: 30008 # If not used in a supported platform, it will act as a NodePort selector: label1: value1Namespaces Namespaces are kubernetes components that allow for isolation of resources in a single cluster. It allows to deploy pods, services and deployments in a logical group that enables easy networking and setting up policies and resource constraints (with ResourceQuota objects ).
Default Namespaces Kubernets starts by default with 4 namespaces:
default Kubernetes includes this namespace so that you can start using your new cluster without first creating a namespace. Without any configuration is the one that kubectl access. kube-node-lease This namespace holds Lease objects associated with each node. Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure. kube-public This namespace is readable by all clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. kube-system The namespace for objects created by the Kubernetes system In production environments, the use of the default namespace is not encouraged
DNS in Namespaces In a namespace, all resources must have an unique name (between namespaces is not mandatory). This enables k8s to create DNS infrastructure that allows pods to reach services by their names. In a inter-namespace context, services can be reached using their FQDN
YAML Definition apiVersion: v1 kind: Namespace metadata: name: ns1 The metadata.namespace field can also be populated in YAML definitions for pods, services and deployments to assign them to a NS directly.
ConfigMap ConfigMap are k8s objects that abstract configuration from pods, making it possible to centralize configuration which can then be injected into different pods as files, environment variables or arguments.
Tip These objects are used to store non-confidential data in key-value pairs. For delicate information Secrets are preferred.
Once created, pod definitions can reference the objects:
Mounted as a volume: Each of the keys in data becomes a file under the mount path As environment files: Particular env variables can reference the config, or the whole config set can be imported. See here for details YAML Definition apiVersion: v1 kind: ConfigMap metadata: name: game-demo data: # property-like keys; each key maps to a simple value player_initial_lives: "3" # file-like keys user-interface.properties: | color.good=purple color.bad=yellow allow.textmode=true Secrets A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Is similar to a ConfigMap but more secure. Once again, once created, pod definitions can reference the objects:
Mounted as a volume: Each of the keys in data becomes a file under the mount path As environment files: Particular env variables can reference the secrets. See here for details Warning Secrets are by default in kubernetes not that secure. They are stored encoded (base64), not encryted, and anyone with access to the cluster can access them. There are measurements that can be taken to reduce the risk (enabling encryption at rest, enabling RBAC rules), but is almost worth to manage secrets with a third party solution like Hashicorp Vault
YAML Definition apiVersion: v1 kind: Secret metadata: name: app-secret data: DB_Host: bXlzcWw= # Each value is stored base64 encoded DB_User: cm9vdA== DB_Password: cGFzd3Jk`,description:"",tags:null,title:"API Objects",uri:"/k8s/api_objects/"},{breadcrumb:"Notes \u003e Kubernetes",content:`Containers are an OS abstraction that allows to solve the following problems:
Multiple services have different dependencies: Libraries that need to be installed at the OS level are most of the times required to run software on machines. Its not rare that two softwares that run together in a server, might need different versions of the same library, or conflicting libraries to work. Environments are different: When developing software its very difficult to guarantee that the development environment (e.g developer workstation) matches the production environment where the app will run. Long setup times: Traditional application deployment involved setting up the servers to accommodate the different applications following concrete steps (normally provided by the developer). This and additional debugging needed eat up a lot of time In this sense, containers provide isolated environments (resources, network) that share the OS kernel. This containers include the applications and all the necessary libraries and dependencies that it needs to run.
Applications that are containerized, are shipped with only additional software needed (on top the kernel). The container technology (e.g docker) acts as an interface between the containers and the host kernel.
This is the reason why Linux based containers cannot run in windows hosts and the other way around
Images In the container context, an image is a package or a template. It can be stored in repositories online and shared and it is used to create one or more containers.
Containers vs VMs Containers offer a lightweight alternative to traditional virtual machines, but in turn, provide less isolation.
Container VMs Share the underlying OS, don’t have to create one every time Each VM has its own OS Can only run if host has similar tech Any VM can run on any host OS Sharing means is only partially isolated from other containers Totally isolated from other VMs Faster to spin up, lighter Heavier and slower Runtimes and CRI The most used container runtime (software to create and manage containers) is Docker. This technology is not a single component but instead a stack.
In this stack, docker provide a lot of “helpful” components on top of of a “lower level” container runtime: containerd
CRI and Kubernetes docker support In order to interact with container runtimes, K8s asks them to implement the CRI (Container Runtime Interface). With this, the kubelet agent can seamlessly communicate with the runtimes.
Docker does not implement the CRI, as its main use case was always “human” users. For this reason, kubernetes implemented an additional component called dockershim that (rather dirtily) allowed interface with docker. However, from versions 1.24 onwards support for dockershim was removed.
containerd however implements this interface (as well as other engines like cri-o or rkt). This means that containerd is used as a drop in replacement for docker in modern kubernetes deployments.
Containerd As explained above, this is a runtime that is used by docker in the backend. However, it can be installed and used on its own. For this different tools exists:
ctr: Is a cli tool included with containerd. Its is not meant for heavy use, and its is not user friendly nerdctl: Is a cli tool that most of the times can be used as drop-in replacement for the docker command crictl: Is a k8s developed CLI tool that allows to interact and manage all CRI compliant runtimes. This allows to inspect and monitor but it is not recommended for creating containers `,description:"",tags:null,title:"Background: Containers",uri:"/k8s/docker/"},{breadcrumb:"Notes",content:"",description:"",tags:null,title:"Terraform",uri:"/terraform/"},{breadcrumb:"Notes \u003e Terraform",content:`Workspaces are used to managed multiple instances of the same infrastructure. They are useful for separating environments (dev, staging, prod) or for managing multiple instances of the same infrastructure (multiple Kubernetes clusters, multiple web apps, etc).
Workspaces are supported by the different backends (local, remote, and enhanced). In terraform cloud (remote backend) these don’t map directly to workspaces in the UI. The “cloud” workspace is used for organization, similar to directories, and the “cli” workspace is used for environment management.
default workspace The default workspace is created automatically when running terraform init. It is used when no workspace is specified.
This workspace cannot be deleted.
Interpolation Workspaces can be used in interpolation expressions. The terraform.workspace variable can be used to get the name of the current workspace.
resource "aws_instance" "web" { ami = "ami-a1b2c3d4" instance_type = "t2.micro" tags = { Name = "web-\${terraform.workspace}" } }CLI Commands terraform workspace list: Lists all workspaces terraform workspace new \u003cname\u003e: Creates and switch to new workspace terraform workspace select \u003cname\u003e: Selects a workspace terraform workspace show: Shows the current workspace terraform workspace delete \u003cname\u003e: Deletes a workspace Mapping to Terraform Cloud Terraform Cloud supports multiple workspaces per project. Depending on the configuration used, those environments can be automatically mapped to CLI workspaces.
Legacy remote backend If using terraform cloud with the legacy remote backend code block, the prefix attribute can be used to map workspaces to CLI workspaces.
terraform { backend "remote" { hostname = "app.terraform.io" organization = "TestJPOrg" workspaces { prefix = "cli-" } } }The example above will use all workspaces that start with cli- as CLI workspaces. They will be shown when running terraform workspace list and can be selected with terraform workspace select \u003cname\u003e.
Cloud block If using the cloud block, the tags or project attribute can be used to map workspaces to CLI workspaces.
terraform { cloud { organization = "TestJPOrg" workspaces { tags = ["cli"] } } }The example above will use all workspaces that have the cli tag as CLI workspaces. They will be shown when running terraform workspace list and can be selected with terraform workspace select \u003cname\u003e. Similarly, the project attribute can be used to map all workspaces in a project to CLI workspaces.
`,description:"",tags:null,title:"Workspaces",uri:"/terraform/workspaces/"},{breadcrumb:"Notes \u003e Terraform",content:`Terraform enterprise is a self-hosted version of Terraform Cloud. It offers enterprise-grade features such as audit logging, SAML SSO, and role-based access control.
It requires a license and can be installed on-premises or in the cloud.
Requirements They depend on the operational mode (how data should be stored):
External services mode: Requires a PostgreSQL database and a storage bucket (S3, GCS, Azure Blob Storage, etc). Mounted disk mode: Requires a mounted disk (NFS, EFS, etc). Demo mode: Stores data in the instance. Not recommended for production. System requirements vary, but some recommendations are:
At least 4 CPU cores At least 8GB of RAM At least 50GB of disk space (40 for docker data and 10 for the application) Terraform enterprise also supports installation on air-gapped environments. This does not require internet access and can be used to install Terraform Enterprise on a private network.
`,description:"",tags:null,title:"Terraform Enterprise",uri:"/terraform/enterprise/"},{breadcrumb:"Notes \u003e Terraform \u003e Language",content:`Terraform has many built-in functions that can be used to transform and combine values. They can be used in expressions and interpolations.
Numeric Functions abs: Returns the absolute value of a number. floor: Returns the largest integer value less than or equal to a number. ceil: Returns the smallest integer value greater than or equal to a number. min: Returns the smallest of one or more numbers. max: Returns the largest of one or more numbers. log: Returns the natural logarithm of a number. pow: Returns a number raised to a specified power. signum: Returns the sign of a number, indicating whether the number is positive, negative, or zero. parseint: Converts a string representation of a number into an integer given a specified base. String Functions chomp: Removes trailing newline characters from a string. format: Returns a formatted string from a format string and a list of values. Similar to printf in other languages. formatlist: Returns a formatted string from a format string and a list of values. Similar to printf in other languages. indent: Indents each line of a string by a given number of spaces. join: Joins a list of strings into a single string using a separator. lower: Returns a copy of a string with all Unicode letters mapped to their lower case. regex: Returns a the first substring matched by a regular expression. regexall: Returns all substrings matched by a regular expression as a list. replace: Returns a copy of a string with all occurrences of a substring replaced with a new substring. split: Returns a list of substrings, separated by a delimiter. strrev: Returns a reversed string. substr: Returns a substring of a string given a starting position and length. title: Returns a copy of a string with all Unicode letters that begin words mapped to their title case. trim: Returns a copy of a string with leading and trailing of the specified characters removed. trimprefix: Returns a copy of a string with the specified prefix removed. trimsuffix: Returns a copy of a string with the specified suffix removed. trimspace: Returns a copy of a string with all leading and trailing white space removed, as defined by Unicode. upper: Returns a copy of a string with all Unicode letters mapped to their upper case. Collection Functions alltrue: Returns true if all the given values are true. anytrue: Returns true if any of the given values are true. chunklist: Returns a list of lists, each containing a subset of the given list of a given size. coalesce: Returns the first non-null value in a given list. coalescelist: Returns the first non-null value in a given list. compact: Returns a copy of a list with all null values removed. concat: Returns a new list by concatenating lists together. contains: Returns true if a given list contains a given value. distinct: Returns a copy of a list with all duplicate elements removed. element: Returns the element of a list at a given index. index: Returns the index of a given value in a list. flatten: Returns a copy of a list of lists, flattened to a single list. keys: Returns a list of all the keys in a map. length: Returns the length of a string, list, or map. lookup: Returns the value of a single element from a map, given its key. If the given key does not exist, a default value is returned. matchkeys: Returns a list of element whose indices match the corresponding indices in another list. Example: matchkeys(["a", "b", "c"], ["A", "B", "C"], ["A"]) returns ["a"]. merge: Returns a map that is the result of merging two or more maps. one: Takes a list of zero or one element and returns the element if it exists, or null if the list is empty. If the list has more than one element, an error is thrown. range: Returns a list of numbers in a given range. reverse: Returns a copy of a list with elements in reverse order. setintersection: Returns a list that contains only the elements that exist in all of the given lists. setproduct: Returns a list of all possible combinations of elements from two or more given lists (Cartesian product). setsubtract: Returns a list that contains only the elements that exist in the first list but not in any of the other given lists. setunion: Returns a list that contains all of the elements of all of the given lists. slice: Returns a subset of a list, given a starting index and length. sort: Returns a copy of a list with elements in sorted order (always strings if input is number is casted) sum: Returns the sum of a list of numbers. transpose: Takes a map of string lists and returns another one with the keys and values swapped. Example: transpose({"a":["1","2"], "b": ["3", "4"], "c": ["1", "3"]}) returns {"1":["a", "c"], "2":["a"], "3":["b", "c"], "4":["b"]}. values: Returns a list of all the values in a map. zipmap: Returns a map by combining two lists into a map (Element in the first list is the key, element in the second list is the value). Encoding Functions base64decode: Returns a string decoded from a base64-encoded string. base64encode: Returns a string encoded to a base64-encoded string. jsondecode: Returns a map or list decoded from a JSON string. jsonencode: Returns a JSON string from a given map or list. yamldecode: Returns a map or list decoded from a YAML string. yamlencode: Returns a YAML string from a given map or list. urlencode: Returns a string encoded to a URL-encoded string. base64gzip: Returns a string encoded to a base64-encoded string and compressed with gzip. Filesystem Functions abspath: Returns the absolute path of a file. dirname: Returns the directory of a file path. pathexpand: Returns the path of a file with the user’s home directory expanded. basename: Returns the file name of a file path. file: Returns the contents of a file as a string. fileexists: Returns true if a file exists. fileset: Returns a list of files for given glob patterns. filebase64: Returns the contents of a file as a base64-encoded string. templatefile: Returns a rendered template from a template file and a set of template variables. Date and Time Functions formatdate: Returns a formatted date string from a timestamp. timeadd: Returns a timestamp that is a given amount of time after a given timestamp. timestamp: Returns a timestamp from a date string. Can be used as input for other date and time functions. Hash and Crypto Functions Functions that return a hashes and cryptographic strings
bcrypt: Returns a bcrypt hash of a given string. Used for shadow password files. uuid: Returns a version 4 UUID. Others like base64sha256, base64sha512, md5, sha1, sha256, sha512, etc. IP Network Functions cidrhost: Returns the host address within a given CIDR range. cidrnetmask: Returns the network mask within a given CIDR range. cidrsubnet: Returns a subnet address within a given CIDR range. cidrsubnets: Returns a list of subnet addresses within a given CIDR range. Type Conversion Functions can: Returns true if no error is thrown when evaluating an expression. default: Returns a default value if an expression returns null. nonsensitive: Returns a non-sensitive value from a sensitive value. sensitive: Returns a sensitive value from a non-sensitive value. tobool: Returns a boolean value from a given value. tomap: Returns a map from a given value. tonumber: Returns a number from a given value. toset: Returns a set from a given value. tostring: Returns a string from a given value. try evaluates a list of expressions and returns the value of the first expression that does not throw an error. `,description:"",tags:null,title:"Functions",uri:"/terraform/language/functions/"},{breadcrumb:"Notes \u003e Terraform",content:`Terraform files (normally with the extension .tf) are written in HashiCorp Configuration Language (HCL). It is a declarative language, meaning that you describe the desired state of the infrastructure, and Terraform will figure out how to create that state.
Basic Syntax The main building block of HCL is the block. It is defined by a block type, a label and a body:
block_type label { key = "value" }Here:
Values can be strings, numbers, booleans, lists or maps. They can also be references to other objects or variables. `,description:"",tags:null,title:"Language",uri:"/terraform/language/"},{breadcrumb:"Notes \u003e Terraform",content:"",description:"",tags:null,title:"Modules",uri:"/terraform/modules/"},{breadcrumb:"Notes \u003e Terraform \u003e Modules",content:`Terraform modules should follow a standard structure to make them easier to use and maintain. The following structure is recommended:
ROOT ├── README.md ├── main.tf ├── variables.tf ├── outputs.tf ├── LICENSE ├── modules │ ├── module1 │ │ ├── README.md │ │ ├── main.tf │ │ ├── variables.tf │ │ └── outputs.tf │ ├── module2 │ │ ├── main.tf │ │ ├── variables.tf │ │ └── outputs.tf │ │ README.md: Contains the documentation of the module and how to use it. main.tf: Contains the resources of the module. variables.tf: Contains the input variables of the module. outputs.tf: Contains the outputs of the module. LICENSE: Contains the license of the module. Nested modules Modules can be nested to create more complex modules. For example, a module that creates a VPC can be composed of a module that creates the subnets and a module that creates the routing tables.
They should be placed in the modules directory in the root module. If they contain a README.md file, it is deemed as a public module usable by external users. If not, it is deemed as a private module only usable by the root module. Relative paths should be avoided when referencing nested modules. `,description:"",tags:null,title:"Standard Structure",uri:"/terraform/modules/standard-structure/"},{breadcrumb:"Notes \u003e Terraform \u003e Modules",content:`Terraform modules can be published on the Terraform Registry. This is the official registry of Terraform modules.
Modules can be of three types:
Official modules: published by HashiCorp Verified modules: published by a verified publisher Community modules: published by anyone When using the search bar on the Terraform registry only verified and official modules are shown by default.
Using modules To use a module, add a module block to the configuration file:
module "vpc" { source = "terraform-aws-modules/vpc/aws" version = "2.9.0" # ... }terraform init will download the module and its dependencies.
Publishing modules The terraform registry is a public index of modules To publish a module, it has to be published to a git repository (mainly GitHub) following a specific structure for the naming of the repository and the files When working with github, the account can be integrated with the terraform registry to automatically publish modules when a new release is created. `,description:"",tags:null,title:"Public Modules",uri:"/terraform/modules/public-modules/"},{breadcrumb:"Notes \u003e Terraform",content:`Terraform can log to a file or to the standard output. The log level can be set to TRACE, DEBUG, INFO, WARN or ERROR.
Logging is controlled by the TF_LOG environment variable. Different log levels can be set for different components by setting the TF_LOG_[COMPONENT] environment variable. For example, TF_LOG_PROVIDER sets the log level for providers and TF_LOG_CORE sets the log level for the core.
The TF_LOG_PATH environment variable can be used to set the path to the log file. If it is not set, the log is written to the standard output.
`,description:"",tags:null,title:"Logs",uri:"/terraform/logs/"},{breadcrumb:"Notes \u003e Terraform",content:`Drift is the difference between the state that is currently in the state file and the real infrastructure.
Drift can be caused by:
manual changes to the infrastructure changes to the infrastructure that were not made with Terraform Managing drift There is three ways to manage drift:
Import the resource into the state file Refresh the state file Replace the resource Replace The -replace flag in apply and plan commands allows to force the replacement of a resource given its address (e.g. aws_instance.web).
This is useful when a cloud resource is damaged and cannot be recovered.
Refresh state The terraform refresh command allows to update the state file with the current state of the infrastructure. It does not modify the infrastructure.
Its an alternative to terraform apply -refresh-only which does not require to have a configuration file. his is the correct way to do this since refresh has been deprecated.
Import The terraform import command allows to import an existing resource into the state file.
For this, a resource must be defined in the configuration file and the terraform import command must be run with the resource address as argument. This resource can have an empty body but it will not be automatically filled with the current state of the resource.
The import command accepts a resource address as argument and the ID of the resource as second argument. What this id means depends on the provider. For example, for AWS, it is the ARN of the resource.
Example:
In the configuration file:
resource "aws_instance" "web" { # ... }then run:
terraform import aws_instance.web i-1234567890abcdef0`,description:"",tags:null,title:"Drift",uri:"/terraform/drift/"},{breadcrumb:"Notes \u003e Terraform",content:`Terraform stores the state of the infrastructure it manages in a file. This state file is extremely important; it maps various resource metadata to actual cloud resource IDs so that Terraform knows what it is managing.
Terraform uses this local state to create plans and make changes to your infrastructure.
Local state is stored in a file named terraform.tfstate.
State File The state file is a JSON file that contains the following information:
Resource IDs Resource metadata Resource attributes Output values Provisioners Module dependencies etc. The state file contains sensitive information like passwords, keys, etc. in plain text and special care must be taken to protect it.
Terraform cloud takes measures to protect the state file like encrypting it at rest and in transit, storing it on memory, etc.
CLI Commands terraform state list: Lists all resources in the state file. terraform state show \u003cresource\u003e: Shows the attributes of a specific resource in the state file. terraform state rm \u003cresource\u003e: Removes a resource from the state file (not destroyed). terraform state mv \u003cresource\u003e \u003cnew-resource\u003e: Moves a resource in the state file. Allows for renaming resources avoiding destroying and recreating them. terraform state pull: Pulls the state and outputs it as JSON. terraform state push: Pushes a local state file to remote state. Backups Terraform commands that modify state will automatically create backups of the prior state in a file called terraform.tfstate.backup. Backends Each Terraform project has a backend that defines where and how operations are performed, where state snapshots are stored, etc.
There is two types of backends:
Standard: Can only store state but not perform any terraform operations. Third party providers (normally storage providers). S3, Azure Blob Storage, Google Cloud Storage, etc. Others like: Consul, etcd, http, postgresql, etc. Enhanced: Can store state and perform terraform operations. Local: Stores state locally in a file and performs operations locally. Remote: Stores state remotely and performs operations remotely. The -backend-config flag can be used with terraform init to specify backend configuration values without having to modify the configuration file. This is useful for storing secrets like access keys and passwords in backends like S3 or http.
Local Backend The local backend stores state locally in a file named terraform.tfstate. Used by default if no backend is specified. Path of the state file can be changed with the configuration block Remote Backend Remote backends store state remotely and perform operations remotely (Terraform cloud, terraform enterprise ) Uses the concept of workspaces to store state for different environments. Workspaces are like different instances of the same infrastructure. Each workspace has its own state file. Workspaces can be used to separate environments (dev, staging, prod) or different components of the same infrastructure (networking, compute, storage, etc.). When configuring a remote backend, a workspace name or a prefix (if using multiple workspaces) must be specified. It can be configured with a backend block:
terraform { backend "remote" { hostname = "app.terraform.io" organization = "TestJPOrg" workspaces { name = "test-wp" } } }But its recommeded to use a cloud block instead:
terraform { cloud { organization = "TestJPOrg" workspaces { name = "test-wp" } } }terraform_remote_state Data Source The terraform_remote_state data source can be used to access the state of a different Terraform project. It exposes the outputs of the root module of the target project. Its argument backend can be used to specify a remote or local backend target Example:
data "terraform_remote_state" "vpc" { backend = "local" config = { path = "../vpc/terraform.tfstate" } } resource "aws_instance" "web" { ami = "ami-a1b2c3d4" instance_type = "t2.micro" subnet_id = data.terraform_remote_state.vpc.outputs.subnet_id } In general is recommended to use normal data sources instead of terraform_remote_state. This is because access to the entire state is required to use terraform_remote_state and this can lead to unintended consequences and security issues. Using normal data sources also provides interaction with live data instead of the data in the state file.
State Locking Terraform uses a state lock to prevent two processes from modifying the state at the same time. It acts similarly to a mutex. It happens automatically when running any command that modifies state. In case the automatic lock fails, Terraform will show an error with the lock ID and the user can manually unlock the state with terraform force-unlock \u003clock-id\u003e. This should only be used as a last resort. `,description:"",tags:null,title:"State",uri:"/terraform/state/"},{breadcrumb:"Notes \u003e Terraform \u003e Language",content:`Expressions are used to refer to or compute values within a configuration. Expressions are used in a number of situations, most commonly in argument values for resources and data sources to express values that cannot be determined until apply time.
Types Primitive Types:
Boolean: true or false Number: 42 or 3.1415 String: "Hello, World" Normally double-quoted, but single-quoted is also valid DOuble quotes allow for escape sequences like \\n for newline and also special escapes like $\${ to include a literal \${ in the string or %%{ to avoid using interpolation and template syntax. Heredoc syntax is also supported for multi-line strings. (\u003c\u003cEOF and EOF) No Type: null (Represents the absence of a value, when wanting to use the default value of an argument)
Complex Types:
List/Tuple: [1, 2, 3] (Ordered, indexed, zero-based) Tuple allows for elements of different types Set: ["one", "two", "three"] (Unique values, unordered, same type(will be casted to match the first element)) Map: { "one" = 1, "two" = 2 } (All values must be of the same type) Object: { one = 1, two = 2, three = "three" } (Similar to a map, but can have different types of values) String Templates Interpolation Interpolation is used to insert the value of an expression into a string. Interpolation is indicated by the \${ sequence, and terminated by the } character. Example:
Name = "HelloWorld-\${var.environment}"Directives Directives allow for conditional results and iteration over collections inside of a string template. Directives are indicated by the %{ sequence, and terminated by the } character. Example:
"Hello, %{ if var.name != "" }\${var.name}%{ else }unnamed%{ endif }!" \u003c\u003cEOT %{ for ip in aws_instance.example[*].private_ip } server \${ip} %{ endfor } EOT Whitespace can be trimmed when using directives by adding the ~ at the end of the sequence. Example: %{ if var.name != "" ~}. Similar to go {{- and -}}.
Operators Arithmetic Operators + Addition - Subtraction * Multiplication / Division % Modulo -a Flip to negative Comparison Operators == Equal != Not Equal \u003e Greater Than \u003e= Greater Than or Equal \u003c Less Than \u003c= Less Than or Equal \u0026\u0026 Logical AND || Logical OR ! Logical NOT Conditional Operators condition ? trueVal : falseVal Ternary Operator Return type of if and else must be the same For Loops for loops can be used to iterate over complex types (lists, sets, maps, objects, tuples) Types of For Loops for s in list: Iterates over a list, tuple or set, assigning each element to the variable s. for k, v in map: Iterates over a map, assigning each key to the variable k and each value to the variable v. for k, v in object: Iterates over an object, assigning each key to the variable k and each value to the variable v. for i, v in list: Iterates over a list,tuple or set assigning each index to the variable i and each value to the variable v. Output type The output type of the expression is determined by the first and last character of the expression. [for ...] Tuple {for ...} Object For this the special operator =\u003e can be used to specify the key and value of the object. Example: { for s in list: s =\u003e upper(s) } Reductions Conditional expressions can be used inside of a for expression to filter the results. Example: [for s in list: s if s != ""] will return a list with all the elements of the list that are not empty strings. Splat Expression Short version of for expression. Normally used to get values from lists, sets and tuples [ for s in list: s.id ] is equivalent to list[*].id When used with anything that is not a list, it will return a list if the value is not null or an empty list if the value is null. This is normally used in for_each arguments to support optional values. Dynamic Blocks Dynamic blocks allow for repeating a nested block within a resource or module. They have a for_each argument that can be used to iterate over a map or set and a content argument that contains the nested block. The type of the block is given in the tag of the dynamic block. The iterator argument can be used to specify the name of the variable that will be used to refer to the current element in the iteration. Otherwise, the name of the block will be used. Example:
The following:
locals { ebs_block_device = [1, 2] } resource "aws_instance" "example" { dynamic "ebs_block_device" { for_each = local.ebs_block_device content { device_name = "/dev/sdh\${ebs_block_device.value}" volume_size = 10 volume_type = "gp2" } } }Is equivalent to:
resource "aws_instance" "example" { ebs_block_device { device_name = "/dev/sdh1" volume_size = 10 volume_type = "gp2" } ebs_block_device { device_name = "/dev/sdh2" volume_size = 10 volume_type = "gp2" } }Version Constraints Version constraints can be used to specify the version of a provider or module. Uses semantic versioning. = Exact version \u003e= Greater than or equal to \u003c= Less than or equal to ~\u003e Greater than or equal to the specified version, but less than the next minor version. Example: ~\u003e 2.0.0 is equivalent to \u003e= 2.0.0, \u003c 2.1 `,description:"",tags:null,title:"Expressions",uri:"/terraform/language/expressions/"},{breadcrumb:"Notes \u003e Terraform \u003e Language",content:`The resource block defines a resource that exists within the infrastructure. It is the most important block in Terraform.
This is a resource whose state is managed by terraform ,contrary to a data source which is read-only. This means this will be created, updated or deleted by terraform.
Meta-Arguments Meta-arguments are arguments that are available on every resource block. They are used to change the behavior of the resource.
depends_on Explicitly specify the order of creation (or destruction) of resources. Used when Terraform cannot infer the correct order of creation (or destruction) of resources. Takes a list of resources that the current resource depends on. Example:
resource "aws_instance" "web" { # ... depends_on = [ aws_db_instance.db, ] }count Create multiple instances of a resource. Exposes a count variable that can be used in other arguments in the block. It must be whole number and known during the planning phase. It can also be an expression. Example:
resource "aws_instance" "web" { count = 2 # ... tags = { Name = "web-\${count.index}" } }for_each Create multiple instances of a resource. Exposes a each variable that can be used in other arguments in the block. Input is either a map or a set. It must be known during the planning phase. It can also be an expression. Example:
resource "aws_instance" "web" { for_each = toset(["us-east-1", "us-west-1"]) # ... tags = { Name = "web-\${each.key}" # because is a set, each.key is the value } } resource "aws_instance" "db" { for_each = { us-east-1 = "db-east" us-west-1 = "db-west" } # ... tags = { Name = each.value Location = each.key } }lifecycle Used to change the behavior of a resource during creation, update, and deletion. It can control actions like ignoring changes to a resource, preventing a resource from being destroyed, or creating multiple instances of a resource. Example:
resource "aws_instance" "web" { # ... lifecycle { ignore_changes = [ # Ignore changes to tags, e.g. because a management agent # updates these based on some ruleset managed elsewhere. tags, ] create_before_destroy = true # if this is replaced, the new instance will be created before the old one is destroyed } }provider Used to specify which provider a resource should use by using the alias of the provider. It can be used to override the default provider configuration. Example:
provider "aws" { region = "us-east-1" } provider "aws" { alias = "east" region = "us-east-1" } resource "aws_instance" "web" { # ... provider = aws.east # use the provider with the alias "east" }`,description:"",tags:null,title:"Resource",uri:"/terraform/language/resource/"},{breadcrumb:"Notes \u003e Terraform \u003e Language",content:`The terraform block is normally the first block in a configuration. It is used to configure the behavior of Terraform itself.
Possible configuration options include:
Required version of Terraform Required providers (the ones that will be downloaded when running terraform init) Backend configuration (where the state file will be stored) Experimental features `,description:"",tags:null,title:"Terraform Block",uri:"/terraform/language/terraform_config/"},{breadcrumb:"Notes \u003e Terraform \u003e Language",content:`Input Variables Input variables are used to parameterize Terraform configurations. They are defined in the root module or a child module.
They are defined using the variable block and normally stored in a different file (e.g. variables.tf).
Each block allows for different arguments:
type: the type of the variable (string, number, bool, list, map, object, tuple, set, any) default: the default value of the variable description: a description of the variable validation: a validation rule(s) for the variable sensitive: whether the variable is sensitive (will not be shown in the output) Example:
variable "region" { type = string description = "The region where the resources will be created" default = "us-east-1" }Variable Definitions Variables can be defined in different ways:
File: Terraform will automatically load files called terraform.tfvars or *.auto.tfvars (e.g. production.tfvars) and read the values defined there into the declared variables. The file uses HCL syntax. Environment variables: Terraform will read any environment variables that start with TF_VAR_ and assign them to the declared variables. The name of the variable is the part after TF_VAR_ (e.g. TF_VAR_region will be assigned to the region variable). CLI Arguments: Terraform will read any command line arguments that start with -var or -var-file and assign them to the declared variables. The syntax is -var="region=us-east-1" or -var-file="production.tfvars". Precedence:
Environment variables \u003e terraform.tfvars \u003e *.auto.tfvars \u003e -var \u003e -var-file
Output Variables Output variables are used to expose information about the infrastructure. They are defined in the root module or a child module.
They are defined using the output block and normally stored in a different file (e.g. outputs.tf).
Each block allows for different arguments:
value: the value of the output description: a description of the output sensitive: whether the output is sensitive (will not be shown in the output but will be plainly visible in the state file) They can be called using the terraform output command. The -json and -raw flags can be used to change the output format and use the command in scripts or as part of pipelines.
Locals Locals are variables that are only used within the configuration. They are not exposed to the outside world.
They can be used to avoid repetition and to make the configuration more readable.
They are declared using one or more locals blocks. Each variable can be assigned a static or computed (result of a function, reference to resource output, etc) value.
Example:
locals { region = "us-east-1" azs = ["a", "b", "c"] } resource "aws_instance" "example" { count = length(local.azs) ami = "ami-abc123" instance_type = "t2.micro" availability_zone = local.azs[count.index] region = local.region }Built-in variables Terraform has a set of built-in variables that can be used in the configuration:
terraform.workspace: the name of the current workspace path.module: the path to the module where the variable is used path.root: the path to the root module path.cwd: the path to the current working directory Inside resource blocks, the following variables are also available:
count.index: the index of the current resource (if using the count argument) each.key/each.value: the key/value of the current resource (if using the for_each argument) self: the resource object itself. It can be used to access attributes of the resource (e.g. self.id) `,description:"",tags:null,title:"Variables",uri:"/terraform/language/variables/"},{breadcrumb:"Notes \u003e Terraform \u003e Terraform Files",content:`Providers are used to interact with the APIs of various platforms. They are used to create, update and delete resources.
Some include:
Cloud providers (AWS, Azure, GCP) Databases (MySQL, PostgreSQL) SaaS Platforms (GitHub, Datadog, PagerDuty) They are separate plugins and are downloaded when running terraform init
TO get an overview of the providers used by a module, run terraform providers
Tier They are divided into three tiers:
Official: maintained by HashiCorp Verified: maintained by a partner of HashiCorp Community: maintained by the community (no guarantee of maintenance) Registry The Terraform Registry is a public index of providers and modules. It is used to discover providers and modules and download them.
Configuration Block The configuration block is used to configure the provider. It is defined at the root of the configuration. Its different for each provider Aliases Aliases are used to configure multiple instances of the same provider (for example, multiple AWS accounts, or different regions) They are created by adding an alias argument to the provider block: provider "aws" { alias = "west" region = "us-west-2" } They can be used in the resource level, module level or parent provider level: resource "aws_instance" "example" { provider = aws.west # ... }module "vpc" { source = "./vpc" providers = { aws = aws.west } }terraform { required_providers { aws = { source = "hashicorp/aws" version = "~\u003e 3.0" configuration_aliases = [ aws.west ] } } }This last declaration specifies that when another module calls this one it must explicitly provide a provider configuration named aws.west, rather than just inheriting a default AWS provider configuration automatically.
`,description:"",tags:null,title:"Providers",uri:"/terraform/terraform-files/providers/"},{breadcrumb:"Notes \u003e Terraform",content:"",description:"",tags:null,title:"Terraform Files",uri:"/terraform/terraform-files/"},{breadcrumb:"Notes \u003e Terraform \u003e Terraform Files",content:`Provisioners are used to execute scripts on a local or remote machine as part of the resource creation or destruction process. (After the resource is created or destroyed)
General options are:
cloud-init Packer Should be avoided if possible. Terraform is not a configuration management tool. Provisioners do things that do not reflect in the state file. This can lead to inconsistencies between the state file and the actual infrastructure. Terraform recommends cloud-init where possible.
Terraform also has built in providers:
local-exec Executes a command locally on the machine running terraform. Used directly in the resource block. Has a required command argument which can receive terraform variables. Other arguments are: working_dir interpreter (shell) environment (pass key value pairs to the command) remote-exec Executes a command on a remote machine via SSH or WinRM. Receives commands in 3 ways: inline (list of commands as strings) script (path to a script) scripts (list of paths to scripts) Requires a connection block to be defined in the resource block. file Used to copy files or directories to a remote machine via SSH or WinRM. Receives files in 2 ways: content (string) source (path to a file or directory) Requires a connection block to be defined in the resource block. Null Resource The null resource is a resource that does nothing. It allows the execution of provisioners outside of a resource block. This is useful for running provisioners that are not directly related to a resource or to run provisioners for blocks that have count set to more than 1. It has a triggers argument that can be used to trigger the execution of the provisioners. This can be used to trigger the execution of a provisioner when a variable changes. Usage of the null provider can make a Terraform configuration harder to understand. While it can be useful in certain cases, it should be applied with care and other solutions preferred when available. One notable use case is to run a wait command to wait for the status of VMs to change. Terraform Data Similar to null resource but does not require any providers Recommended over null resource `,description:"",tags:null,title:"Provisioners",uri:"/terraform/terraform-files/provisioners/"},{breadcrumb:"Notes \u003e Terraform \u003e Terraform Files",content:"providers terraform providers are responsible for understanding API interactions and exposing resources they are configured within the terraform block (version) and the provider block (specific provider configuration) input variables variables are used to parameterize terraform configurations they can be read from different sources: command line flags environment variables files (e.g. tfvars) locals locals are variables that are only used within the configuration they are not exposed to the outside world outputs outputs are used to expose information about the infrastructure they can be read from the state file they can be used as input for other configurations ",description:"",tags:null,title:"Basics",uri:"/terraform/terraform-files/basics/"},{breadcrumb:"Notes \u003e Terraform",content:`Terraform Cloud is a SaaS offering that provides remote state storage, version control integration, and additional features for teams to collaborate on changes to infrastructure.
Its free tier is limited to 5 users and 3 concurrent runs. State files are stored in a private bucket on S3. Terraform scripts run in a containerized environment. This means that variable definitions are done directly in the UI and not in a separate file. Cost estimation is available for AWS, Azure, and GCP. Sentinel policies can be added to make sure that deployments are compliant with company budget policies. A .terraformignore file can be used to ignore files and directories when uploading to Terraform Cloud. If not present, Terraform will ignore the .terraform and .git directories by default.
Basic Terminology Organization: A collection of workspaces and users. Each organization has one or more owners which can manage the organization and its workspaces. Workspace: Represents a unique environment or stack. Allow for more granular permissions including run level permissions (read runs, apply runs, destroy runs, etc). Can run different versions of Terraform. Team: A group of users that share access to workspaces. Run: A single execution of Terraform (operating on an execution plan).Can be initiated manually or automatically (via webhook, API, or VCS integration). Workflows Terraform cloud supports 3 different ways to work with Terraform projects:
CLI-driven workflow: Terraform CLI is used to create and manage infrastructure. State is stored remotely in Terraform Cloud. VCS-driven workflow: Terraform cloud is integrated with a VCS provider (GitHub, GitLab, Bitbucket). Merge request trigger plan runs and merges to master trigger apply runs. API-driven workflow: Runs are triggered via an API call. This is useful for integrating Terraform with other tools. API Tokens API tokens are used to authenticate with Terraform Cloud. They can be created programmatically, via the UI, or via the CLI.
There are three types of tokens:
User token: Used to authenticate as a user. Has the same permissions as the user who created it. Team token: Used to authenticate as a team. Has the same permissions as the team that created it. Only one team token can be active at a time. Organization token: Used to authenticate as an organization. Only recommended for workspace and team provisioning. Has permissions to manage all workspaces and teams in the organization. Migrating from local state to remote state Create a new workspace in Terraform Cloud Add the remote backend configuration to the terraform block: cloud { organization = "TestJPOrg" workspaces { name = "test-wp" } } Run terraform init to initialize the backend Private Module Registry Terraform Cloud provides a private module registry that can be used to store and share modules across an organization. Modules can be uploaded via the UI or via the API.
This allows to have modules always available and up to date for all users in the organization. It also includes some extra features like versioning, configuration designer, and documentation.
Cloud agents Similar to gitlab runners. It allows self managed machines to pull and run terraform runs. Also available as a docker container Sentinel Sentinel is a policy as code framework that allows to define policies to enforce compliance and security requirements. It is integrated with Terraform Cloud and can be used to enforce policies on Terraform runs.
Policies are defined and treated as code. They are written in the Sentinel language and can be versioned and stored in a VCS repository.
Terraform cloud can automatically integrate policies from a VCS repository. This allows to have a single source of truth for policies and to have them versioned and auditable. These can then be applied to workspaces and runs.
`,description:"",tags:null,title:"Terraform Cloud",uri:"/terraform/terraform-cloud/"},{breadcrumb:"Notes \u003e Terraform",content:"init downloads the provider plugins and modules and initializes the backend stores them in the .terraform folder creates a dependency lock file to enforce the same versions of the providers and modules (.terraform.lock.hcl) with the -upgrade flag, it will upgrade the providers and modules to the latest version that matches the version constraints in the configuration get downloads only modules and stores them in the .terraform folder validate validates the syntax of the terraform files checks for required variables and provider configuration does not check for correctness of the code (e.g. if a resource exists) fmt formats the terraform files with the --diff flag, it will show the changes that will be made console opens an interactive console to test expressions plan creates an execution plan that shows what will be done when apply is called shows what resources will be created, modified or destroyed shows the order of operations the -out flag can be used to save the plan to a file that can be used with apply to apply the plan show Reads and outputs a Terraform state or plan file in a human-readable form If no path is specified, the current state will be shown. apply creates, modifies or destroys resources creates a state file that contains the current state of the infrastructure (local or remote) destroy destroys all resources that are defined in the configuration does not remove the state file ",description:"",tags:null,title:"Commands",uri:"/terraform/commands/"},{breadcrumb:"Notes \u003e Terraform",content:`terraform is an open source and cloud agnostic tool declarative files written in hcl
terraform cloud saas remote state storage vc integration workflows collaboration on changes
lifecycle write terraform code init (get dependencies) plan (dry run) validate apply change management procedure that will be followed when resources are modified (change) change automation is creating a constient way of managing changings terraform uses Execution Plans and Resources graphs
plans manual review of changes that needs to be approved can be visualized using the terraform graph command and piping to GraphViz terraform graph | dot -Tsvg \u003e graph.svg this graph shows the dependencies between resources and determines order of deployment, what to refresh, etc. `,description:"",tags:null,title:"Basics",uri:"/terraform/basics/"},{breadcrumb:"Notes \u003e Terraform",content:`What is infrastructure as code (IaC) Writing configuration (in the form of a script) to automate
creation update destruction of cloud infra Allows to share, version and inventory infra
Manually creating infra is error prone, time consuming and not repeatable.
popular iac tools two types Declarative What you see is what you get Explicit Scripting language json yaml, hcl No misconfiguration ARM Templates Azure Blueprints CloudFormation Terraform Imperative Say what you want the rest is filled in Implicit Less verbose Programming languages CDK AWS pulumi With an imperative tool, you define the steps to execute in order to reach the desired solution. With a declarative tool, you define the desired state of the final solution, and the automation platform determines how to achieve that state.
Infrastrcuture Lifecycle Concept of having clearly defined and distinct work phases which are used by DevOps Engineers to plan, design, build, test, and deliver, maintain and retire cloud infrastructure.
Commonly used Day0 (plan/design), DAy1(develop/iterate/test), Day2(live deployment, maintenance)
IaC brings into the lc: reliabiliy idempotent (always same result) consistent predictable manageability reviews versions
prov, deploy, orchestration provisioning: prepare a server with data, software to make it “ready” deployment: putting an application on a prepared server orchestration:: mostly related to containers, coordinating deployments
drift provisioned infra has a configuration different from the state can be caused by: manual config changes malicious actors errors detect: cloud provider tools like Azure policy using terraforms state as a base to compare fix: remediation in provider tools terraform apply -refresh-only and plan commands manually making the states equal prevent: Immutable infrastructure, always create and destroy, never reuse, Blue, Green deployment strategy.​ GitOps
https://developer.hashicorp.com/terraform/tutorials/state/resource-drift
GitOps Git repo for IaC Review and accept changes Trigger deploy Immutable infrastructure using something like packer to guarantee configuration makes resilent against: cloud resource failures region failures Makes deployment quicker and repeatable (one to one guarantee in all machines, no different versions of dependencies) `,description:"",tags:null,title:"IaC Concepts",uri:"/terraform/iac-concepts/"},{breadcrumb:"Notes",content:"",description:"",tags:null,title:"Go",uri:"/go/"},{breadcrumb:"Notes \u003e Go",content:"",description:"",tags:null,title:"Language Basics",uri:"/go/language-basics/"},{breadcrumb:"Notes \u003e Go \u003e Language Basics",content:`Types Go is a statically typed language. This means that variables are assigned a type when created and can only hold values of that type. This helps with avoiding runtime errors by catching them at compile-time
Go supports several types. The most common being
Type Description Zero Value int Integer value, can be 64 or 32 bits depending on the system it runs on 0 bool Boolean value false string String of UTF-8 characters "" slice Growable list of a type nil map Key-value data structure nil struct A collection of named attributes (Similar to objects in other languages) Empty struct interface A type that holds a value with defined methods nil pointers A type that stores an address in memory of a variable not the variable nil channels A pipe (buffered or not) for sending data asynchronously nil Other types include more control over the size of the variables, as well as some particular use cases like complex numbers:
Type Description Zero value uint Unsigned 32 or 64 bit integer 0 uint8 Unsigned 8 bit integer (0-255) 0 uint16 Unsigned 16 bit integer (0-65535) 0 uint32 Unsigned 32 bit integer (0-4294967295) 0 uint64 Unsigned 64 bit integer (0-18446744073709551615) 0 int8 Signed 8 bit integer (-128-127) 0 int16 Signed 16 bit integer (-32768-32767) 0 int32 Signed 32 bit integer (-2147483648 - 2147483647) 0 int64 Signed 64 bit integer (-9223372036854775808 - 9223372036854775807) 0 float32 32 bit floating point (decimal) values 0 float64 64 bit floating point (decimal) values 0 complex64 Complex numbers with real and imaginary part float32 (0+0i) complex128 Complex numbers with real and imaginary part float64 (0+0i) byte Alias for uint8 0 rune Alias for int32 0 uintptr Unsigned integer large enough to hold any pointer address (32 or 64 bit). 0 array Fix list of items [] function A function can be stored in a variable (functional programming) nil Declaring variables Go supports two types of variable declaration:
Creation and assignment: This type can be used to declare variables at the package and function levels. If using just the creation statement (var i int) the variable will have the corresponding zero value var i int // Create for a given type i = 3 // Assign the value var j = 3 // Infers the type Single statement: This can only be used in function levels i := 3 // Creates and infers the type, then assigns the value Redeclare variables Once declared a value cannot be redeclared in the same scope, only a new value can be assigned, so the following is not valid
func main() { i := 3 i := 5 } Scope of variables Depending on where they are declared variables can be “visible” to different scopes:
Package scoped: When declared at the package level, it can be seen by the entire package package oe var word = "hello" func afunction(){} Function scoped: Only visible inside a particular function func afunction(){ var i = 3 } func another() { // i is not accessible } Statement scoped: Only visible for an statement (loop, if) inside a function func af() { for i := 0; i \u003c 10; i++ { // i is accessible } // i is not visible }Shadowing Redeclaring variables in different scopes can affect the values that can be read. This is a very common bug know as variable shadowing. Below an example
package main var word = "hello" func main(){ var word = "world" // In this scope word is world } func other(){ // In here word is hello }`,description:"",tags:null,title:"Variables",uri:"/go/language-basics/variables/"},{breadcrumb:"Notes \u003e Go \u003e Language Basics",content:`Similar to libraries or modules in other languages, go uses the concept of packages to refer to reusable blocks of code that can be imported and used in other code.
Working with packages is straight forward. One can:
Declare packages: When structuring an application, to split the functionality logically Import packages: While coding to use functions from the standard library, third party packages or even other packages in the app Declaring packages At the beginning of each .go file, the package of the file needs to be declared. It must be the first code to appear and can only be preceded by comments (most of the times documenting the package). Here is an example:
// Package example contains a lot of invisible functions and variables package example Files in a package All files in a directory must be in the same package. Most commonly the package is named after the directory where the files are.
Importing packages Packages need to be imported to be used, this is done always in an import block at the beginning of the file:
// This package will import some other package example import ( "fmt" "github.com/someguy/apackage" other "github.com/otherguy/apackage" )In the example above:
fmt is a package from the stdlib github.com/someguy/apackage is an example of a third party package github.com/otherguy/apackage is another third party package imported with the alias other to avoid naming conflicts when calling apackage.afunction Used packages Go has compile rules that enforce that every imported package must be used. This helps optimize binary sizes. An option to avoid this, is to use a side effects import, which just loads a package. This should always be done in the package main like
import ( "fmt" _ "sync" // Just for some effect ) `,description:"",tags:null,title:"Packages",uri:"/go/language-basics/packages/"},{breadcrumb:"Notes \u003e Go",content:`The Go teams offers the go playground. This is a web code editor and compiler that allows to write and run go code.
The Go Playground has several features that make it a useful tool:
Easy to use: The Playground has a simple interface that makes it easy to write and test out code.
Interactive: The Playground allows you to write code and see the results in real-time, making it a great tool for learning and experimenting with Go.
Shareable: You can share your code snippets with others by sharing the URL of the Playground page. This makes it easy to collaborate with others and get feedback on your code.
Code formatting: The Playground automatically formats your code as you type, making it easier to read and understand.
Import support: You can import packages and libraries from the Go standard library or from third-party sources, allowing you to explore and experiment with a wide range of Go code.
`,description:"",tags:null,title:"Playground",uri:"/go/playground/"},{breadcrumb:"Notes \u003e Linux \u003e Using a Linux System",content:`Below is a list of common operations to check the current status of a linux system
Hardware Some general information about the machine can be obtained by using the uname command. With the -a flag it will print:
Kernel Name Node hostname Kernel release and version Processor type Operating system uname -aChecking uptime The uptime command can be used to determine how long has the server been “up”
uptime --prettyMemory To check the physical available memory use:
grep MemTotal /proc/meminfoHowever, running free gives a more complete overview on memory usage:
free -hCPU The command lscpu displays information about the CPU architecture:
lscpuWith the extended flag, the information can be compressed in a table
lscpu --extendedFor more level of detail, the respective /proc file can be read and grepped
cat /proc/cpuinfoDisks and filesystems The lsblk command is used to display information about block devices (i.e., hard drives, SSDs, etc.) on the system. It displays information in the “lower” level of devices which may or not be mounted.
lsblkTo look into disk space and utilization, the df command is used. This displays information about the amount of space used and avaiable in filesystems that are mounted in the system.
df -hSome of the most commonly used options for df include:
h: Display disk space in a human-readable format. T: Display the filesystem type. i: Display the number of inodes (i.e., the number of files and directories) on the filesystem. To go more into detail about the space used by particular files or directories, du is used.
du [options] [file|directory]Some frequently used commands are:
du -h dir Shows the size of all files in the current directory in a human readable format du -sh dir Same as above, but limits the results to a single directory (instead of recursive) du -ah dir includes hidden files du -h --max-depth=N dir List the human-readable sizes of a directory and any subdirectories, up to N levels deep. Networking Some tools can be used for general networking checking:
ip is a tool that allows to show / manipulate routing, devices, policy routing and tunnels. ip address or ip a allows to show, remove and manage the ips of the different interfaces. THe following command summarizes the interfaces in the system and their assigned IPs
ip -brief a Checking for open ports To check (or verify) open ports can be useful in determining if a service is running or not (e.g. web server).
The first option to do this is with the ss (sockets statistics) tool. This tool allows to not only display information but kill socket connections. To display open ports the following command can be used
sudo ss -tulpnwThis will show all TCP (t) and UDP (u) ports that are open and listening (l). It will display the process (p) using them and their ports in a numeric way n. Finally it will display everything in wide format.
Filtering by port Is possible to use the ss command like ss -ltpn src :1313 for example to see only the entry for the 1313 port in TCP
An alternative is to use nmap. This a very powerful tool used, among other things, to scan a network and discover hosts and services. To use it for finding out open ports, is enough to point it to the localhost ip:
sudo nmap 127.0.0.1This will scan the machine and summarize the ports open and the services listening
Checking firewalls ufw (Uncomplicated Firewall) and iptables are both firewall tools that can be used to secure Linux systems. UFW is a frontend for iptables that simplifies the process of configuring and managing a firewall.
To enable the firewall:
sudo ufw enableThen, adding rules can be done with either allow or deny followed by the port and the protocol. If no protocol is specified it will apply for both
sudo ufw allow 80/tcp #Allow TCP connections on port 80 sudo ufw deny 22 #Deny TCP and UDP connections on port 22 sudo ufw allow out 3306/tcp #Allow only outbound TCP traffic from port 3306 Default By default, if there are no allow or deny rules in ufw, all incoming traffic is denied, and all outgoing traffic is allowed
The status and the rules can be checked at any time with
sudo ufw statusSoftware Distribution Two reliable ways of checking which distribution is a machine running are:
First, the /etc/os-release file. This provides information about the OS in the form of variables that can be sourced. An example would be:
#!/bin/bash # source the os-release file source /etc/os-release # check if the distribution is Ubuntu if [ "$NAME" == "Ubuntu" ]; then echo "This is Ubuntu" fi # check if the distribution is CentOS if [ "$NAME" == "CentOS Linux" ]; then echo "This is CentOS" fiSecond, the lsb_release command can be used to obtain distribution information. An example below:
#!/bin/bash # use lsb_release to get distribution information DISTRIB_ID=$(lsb_release -si) DISTRIB_RELEASE=$(lsb_release -sr) # check if the distribution is Ubuntu if [ "$DISTRIB_ID" == "Ubuntu" ]; then echo "This is Ubuntu" fi # check if the distribution is CentOS if [ "$DISTRIB_ID" == "CentOS" ]; then echo "This is CentOS" fi lsb_release availability The lsb_release command is not installed by default in CentOS and other distributions. Debian, Ubuntu, Mint, openSUSE and Fedora all include it. For red-hat based systems, the package redhat-lsb-core provides it.
Installed packages Checking the packages installed in the system depends on the family of the installed distribution.
Debian based (APT) In debian, ubuntu and related distributions, the list of installed packages can be generated in two ways:
First using the low level dpkg command
dpkg -lSecond using the higher level apt:
apt list --installed Search in sources apt also offers a command to search for packages that might not be installed but are available in the configured sources. This can be done with apt search name or apt-cache search name
In addition, apt show can be used to obtain information about installed (or not installed) packages
Red Hat Based (YUM) In RedHat, CentOS and related distributions, the list of installed packages can be generated in two ways:
First using the low level rpm command
rpm -qaSecond using the higher level yum:
yum list installed Search in sources yum also offers a command to search for packages that might not be installed but are available in the configured sources. This can be done with yum search name
In addition, yum info can be used to obtain information about installed (or not installed) packages
Processes To see the processes running in the system, the main tool is ps. This non-interactive tool, has several options to show information about running processes. The syntax of the arguments can be either standard or BSD.
Some useful commands are presented below:
ps #Shows processes in current shell ps -ef #Shows all processes in the system with full format ps aux #BSD Argument style, shows all processes of all users ps auxww #List all running processes including the full command string ps aox pid,ppcpu #BSD Argument style, shows only PID and %CPU ps -o ppid= -p 0000 #Get the parent PID of a process ps --sort size #Sort processes by memory consumptionThere is also ways to check processes interactively mainly with top and htop. These are real-time tools that show the status of the system. Just running them executes them and menus inside allow to configure the output.
Help top and htop are highly configurable, and their output can be changed live. To figure out how, type h in top or F1 in htop to seek help and see possible commands
Services (Daemons) A system service or daemon is a program that runs in the background on a Linux system to provide functionality or perform a task, such as managing network connections or serving web pages. In order to monitor them and manage them, most distributions use systemctl
To check the status of a service run
systemctl status serviceIn a similar way, systemctl is-active, systemctl is-failed and systemctl is-enabled can be used to check a service.
To list all services active in the system run
systemctl list-unitsFilters can be applied to this command. A very common one is to filter just services:
systemctl list-units --type=serviceFinally to read a service definition can be done with:
systemctl cat serviceMore detailed information can be found in this article
`,description:"",tags:null,title:"Checking the System",uri:"/linux/using-a-linux-system/checking-the-system/"},{breadcrumb:"Notes \u003e Git",content:"",description:"",tags:null,title:"Collaboration",uri:"/git/collaboration/"},{breadcrumb:"Notes",content:`What is? Git is a version control system. In other words it registers changes on a set of files.
Good to know Git uses SHA-1 to reference a version of a file or files.
Basic Workflow The basic workflow of Git is based on 3 states that files can be at:
The file (or files) are in the working directory, where they are modified Changes (modified files) are added to the staging area Staged changes are then committed to the git repository. This creates a version of the file (with an assigned SHA-1 id) called a commit. `,description:"",tags:null,title:"Git",uri:"/git/"},{breadcrumb:"Notes \u003e Git \u003e Collaboration",content:`There are several repository management tools that allow to take the git workflow and ease collaboration based on the cloud. These are hosting platform for git repositories and their use is supported by native git commands. The most popular are:
Github Gitlab Bitbucket Basic Worflow When working with remotes there is 3 general operations performed:
Setup Get data Post data When creating a new project or linking a local repo to the cloud for the first time, an empty remote should be created, after there is two options:
If a local repo with data exist:
Add the remote Push the data If its a new project
Clone the empty remote Do changes Push the data Setup Any git repository can be configured with a remote. In order to get a list of the remotes of a repository,the following command is used
git remote -vOne (or more) can be added to the repository, renamed or deleted
Adding a remote To add a remote the command is:
git remote add \u003cname\u003e \u003curl\u003eWhere:
\u003cname\u003e is the name to give the remote. In most cases, when adding the first remote this is called origin but is just a convention \u003curl\u003e is the HTTPS or SSH url of the repository in Github, gitlab, etc. SSH Setup In order to set up SSH Authentication, an SSH Key needs to be created and set up in the platform (github,gitlab, etc). This allows “password-less” flow which is way nicer than having to input the password every time a commit is pushed.
Main vs master Gihub and gitlab among others are pushing for using the term main instead of master for the main branch of projects. In that sense, any new repo created will have the main branch if created with a file (for exaple a README). To adapt this, git branch -M main can be run to change the local master branch’s name before pushing
Renaming a remote The name of a remote can also be changed after creation with
git remote rename \u003cold\u003e \u003cnew\u003eDeleting a remote To delete a remote, the command is:
git remote remove \u003cname\u003eGetting Data Cloning a remote To “pull” a remote for the first time to a machine that does not have it, the clone command is used.
git clone \u003curl\u003eThe command :
Copies the data from the remote Initializes a local git repo. Sets up remote tracking branch references pointing to the branches of the remote Remote tracking branches These are a pointer (just like any other branch) to the latest commit of a branch in the remote. For example origin/main points to the last known commit of the main branch in the remote origin. To see all remote tracking branch references in a repo, the command git branch -r helps. Its also worth noting that they can be checkout with git checkout origin/main which will show the state of the main branch in detached HEAD mode
Other branches When cloning, only the default branch (master or main) is accessible in the local repository. This is because the cloning process, although it creates RTB references for all remote branches, just “links” the one of the default branch to a local branch.
In order to access other remote branches locally is enough to use
git switch \u003cbranch_name\u003ewhere \u003cbranch_name\u003e is the same name as one of the remote branches. This will automatically:
Create a new local branch called \u003cbranch_name\u003e Link it with the RTB references for the remote branch Fetching git fetch allows to retrieve the latest changes from a remote repository without merging them into the local branch.
The command can be used as git fetch \u003cremote\u003e to fetch all available changes (all branches) or git fetch \u003cremote\u003e \u003cbranch\u003e to fetch a specific branch.
In more detail, it only updates the RTB references but not the local branches. So if running git fetch origin master, the origin/master reference will now point to the latest version, but the local master branch won’t be affected.
Seeing the changes Once fetched, the changes can be seen with git checkout in detached HEAD mode
Pulling git pull is similar to git fetch, but it also merges the changes from the remote repository into the local branch. This means that your local branch is automatically updated with the latest changes from the remote repository.
The command can be used like git pull \u003cremote\u003e \u003cbranch\u003e to pull changes from a specific branch. In addition, git pull will default the remote to origin and the branch to the RTB reference linked to the current branch.
In other words, it updates both the RTB reference (e.g. origin/master) and the current HEAD (e.g master)
Because of this, is critical to run the command in the correct place, as the current branch when the command is run will be the one updated.
fetch vs pull The main difference between git fetch and git pull is that git fetch only retrieves the changes from the remote repository, while git pull retrieves the changes and merges them into your local branch.
Pushing data In order to push or send local changes (commits) to the remote, the command push is used
git push \u003cremote\u003e \u003cbranch\u003eThis command wil create a new branch on the remote (the first time) and push the changes on the local branch with the same name. For example git push origin master will push the changes from the local master branch to the remote master branch
To push to a branch with a different name, the whole syntax can be used:
git push \u003cremote\u003e \u003clocal_branch\u003e:\u003cremote_branch\u003e Upstream setup If the -u is added to the git push command, the remote branch is set and remembered as the “upstream” for the local branch. This means that git will link the two of them, so next time just git push will work.
`,description:"",tags:null,title:"Working with remotes",uri:"/git/collaboration/remotes/"},{breadcrumb:"Notes \u003e Linux \u003e System \u003e Files",content:`What is it? A file system in Linux is the way in which files are stored, organized, and managed on a Linux operating system. It is responsible for managing the data on a storage device, such as a hard disk or solid-state drive, by dividing it into multiple sections or partitions. The file system determines the structure and organization of the files, as well as the methods used to access and modify them.
Info The most commonly used file systems in Linux are the ext4, btrfs, and xfs file systems. The file system chosen depends on the specific requirements of the system, such as performance, scalability, and reliability.
Partitions and multiple file systems In linux each filesystem occupies a disk partition. These are separated “logical drives” or sections inside a single (or more) real drives, but appear as different drives to the OS. Data is stored into different partitions for many reasons, including security or backup.
lsblk provides a tree-like view of the storage devices and their associated partitions, making it easy to see the hierarchical relationships between the different components of the system’s storage.For a graphical interface gparted can be used to check out the partitions in a system.
Mounting filesystems Different file systems are mounted on the filesystem tree. The mount points are just directories in which the new filesystem will live.
The utility mount is used for this task:
sudo mount /dev/sda5 /home #Mounts the filesystem in the sda5 device(partition) in /home sudo umount /home #Dismounts the filesystemThis is a temporal way of mounting a filesystem and will get unmounted in reboot. In order to make it permanent, the etc/fstab file needs to be modified.
Add a new line to the file with the following format:
\u003cdevice\u003e \u003cmount point\u003e \u003cfile system type\u003e \u003coptions\u003e \u003cdump\u003e \u003cpass\u003eFor example, to mount the file system located at /dev/sdb1 with the file system type ext4 to the mount point /mnt/data, the line would be:
/dev/sdb1 /mnt/data ext4 defaults 0 0More information is available in man fstab
Good to know mounted alone, as well as df -Th will show all presently mounted filesystems
NFS A special kind of filesystem is a Network File System. The Network File System (NFS) is a protocol that allows a computer to share its files with other computers over a network. NFS enables seamless access to remote file systems as if they were local, allowing users to access and manage files on remote systems with the same ease as they do on their own computer.
It has a server and a client side
Server On the server, nfs runs a a daemon, and allows to share a directory in the server’s filesystem via NFS (with a network address)
sudo systemctl start nfs # Starts the NFS daemonOn the /etc/exports file, the directories that are going to be shared are typed in. For example /projects *(rw) means that the /projects directory in the server will be shared (with read and write accesses).
Using exportfs -av notifies Linux of the changes. The NFS daemon can also be reset, and set to start on boot.
Client On the client, the filesystem is mounted like any other (Is also possible to modify fstab to boot with the filesystem mounted)
sudo mount servername:/projects /mnt/nfs/projects`,description:"",tags:null,title:"Filesystems",uri:"/linux/system/files/filesystems/"},{breadcrumb:"Notes \u003e Linux \u003e System \u003e Files",content:`The Linux file system structure is organized into a tree-like hierarchy, starting from the root directory /. The root directory contains several standard subdirectories, each serving a specific purpose.
Good to know Similar information can be found using man hier
Here is a list of the standard folders in the Linux filesystem hierarchy:
/bin - Binary executables for system \u0026 users. This directory contains essential command line utilities, such as ls, cat, and cp, that are required for basic system operation and are accessible by both the system and users.
/sbin - Binary executables for system administrator only. This directory contains utilities that are necessary for system administration, such as init, fdisk, and ip. These utilities can only be executed by the root user or other users with administrative privileges.
/etc - Configuration files for system \u0026 applications. This directory contains configuration files for the system, as well as for applications and services installed on the system. These files are often human-readable and can be edited to modify the behavior of the system and its applications.
/dev - Device files for attached devices. This directory contains special files, also known as device files, that represent and provide access to the various devices attached to the system, such as hard drives, keyboards, and printers. The directory is initially empty, when not mounted. udev which manages devices in linux, then creates entries dynamically when devices are found
/lib - Libraries for executables in /bin \u0026 /sbin. This directory contains libraries and shared objects required by the executables in the /bin and /sbin directories. Libraries are collections of code that can be reused by multiple programs, improving system efficiency.
/boot - Files for system booting. This directory contains files necessary for booting the system, such as the Linux kernel, initial RAM disk, and boot loader configuration files.
/var - Files that change frequently (logs, spool, temp). This directory contains files and directories that change frequently, such as logs, spool directories, and temporary files. These files are usually deleted or recreated every time the system is restarted.
/tmp - Temporary files deleted on restart. This directory contains temporary files that are deleted when the system is restarted. These files are used by applications and services to store data temporarily.
/usr - User-related data (apps, libs, docs, user homes). This directory contains files and directories related to users, such as user-installed applications, libraries, documentation. This directory contains non essential binaries, and has its own tree, this means, it has /usr/bin, usr/sbin, etc.
/home - Home directories of users. This directory contains a subdirectory for each user on the system, which serves as that user’s home directory. The home directory typically contains personal configuration files, documents, and other files specific to each user. The only exception is the root user, whose home is /root
/media - Mount points for removable media. This directory contains subdirectories for mounting removable storage devices, such as USBs. Each mounted device is represented by a subdirectory in /media, allowing for easy access to the files and directories on the device. In modern distributions,can be named /run
/mnt - Mount points for file systems temporarily mounted. This directory is similar to /media, but it is used for temporarily mounting file systems, such as network file systems, that are not meant to persist across reboots.
/opt - Optional software installation. This directory is intended for the installation of optional software packages that are not part of the standard Linux distribution. Each software package is installed in its own subdirectory within /opt, making it easy to manage and remove the software if necessary.
/proc - Process information file system. This directory is a virtual file system that provides information about the system and running processes. The files in this file system are not stored on disk like traditional files, but are generated on-the-fly by the kernel. The /proc file system is used to provide information about system resources, such as memory usage, system uptime, and CPU utilization, as well as information about running processes, such as their process ID, memory usage, and status.
`,description:"",tags:null,title:"Filesystem Architecture",uri:"/linux/system/files/filesystem-architecture/"},{breadcrumb:"Notes \u003e Git",content:`Git offers different commands that allow to “travel back in time” or undo changes
git checkout Detached HEAD Using git checkout \u003cCOMMIT\u003e leads to Detached HEAD state.
This state, refers to when HEAD, instead of pointing to a branch reference (like usually) points to a specific commit.
This state is useful for:
Looking around the state of a repo Make experimental changes They can be committed and discarded just by switching to a branch They can be kept if creating a new branch from the detached HEAD The last option, allows to branch out on a particular commit in the repo history by reattaching the HEAD
Tip Commits can also be referenced from HEAD instead of the commit hash. So git checkout HEAD~1 takes HEAD back one commit, and so on.
Discard changes git checkout HEAD \u003cfiles\u003e will revert all files to the HEAD (To the last commit made). Another way to do the same thing is git checkout -- \u003cfiles\u003e
git restore This new command was introduce to limit the wide functionality of git checkout. It can do two things:
Discard changes Same functionality as git checkout HEAD \u003cfiles\u003e is achieved with
git restore \u003cfiles\u003e Good to know Restore also allows to set up a “source” different than HEAD to restore files like git restore --source HEAD~1 \u003cfiles\u003e
Unstage files To remove a file from the staging area and take it back to the working directory:
git restore --staged \u003cfile\u003egit reset This commands resets the state of the current branch to an specific commit. It comes in two flavors:
git reset \u003cCOMMIT\u003e: Will delete the commits but keep the changes in the working directory. This is specially useful when commits were added to the wrong branch git reset --hard \u003cCOMMIT\u003e. Will delete the commits and remove the changes git revert Has a very similar effect to git reset, but this command creates a new commit that reverts the changes of another commit :
Important When dealing with collaboration workflows, it is recommended to use revert instead of reset to undo changes. This way, the history that other people have does not have to be rewritten which can lead to issues
`,description:"",tags:null,title:"Undo Changes",uri:"/git/undo-changes/"},{breadcrumb:"Notes \u003e Git",content:`What is stashing? git stash temporarily shelves (or stashes) changes made in a branch, that are not yet commited. This ways, you can store the changes without making a commit and switch context (or branch) to work on something else.
It avoids conflicts and taking changes with you when you switch branches
How to work with stash? The git stash works like a traditional stack. This means is a LIFO data structure in which data can be pushed or popped. In this case, data being the changes made.
git stash save Save uncommitted changes (staged and unstaged) into the stash Also works with git stash git stash pop Get the top of the stack (most recent stashed changes) and apply them to the working directory Removes the changes from the stash git stash apply Similar to pop but does not remove it from stash Useful to apply changes in multiple places git stash list When calling git stash multiple times, all changes go the the stack git stash list shows all stashes and the last commit when they were saved If the changes were saved with git stash save "message",it will also show the message. One can also apply or pop a particular stash with git stash apply stash@{2} A particular stash can also be dropped with git stash drop stash@{2} git stash clear drops all stashes `,description:"",tags:null,title:"Stashing",uri:"/git/stashing/"},{breadcrumb:"Notes \u003e Git",content:`What is a diff? A diff is the output of using the git diff command. Is a text that allows to view differences or changes between commits, branches, files, working directory and many other things.
Example:
diff --git a/rainbow.txt b/rainbow.txt index 0b75516..26ec8e7 100644 --- a/rainbow.txt +++ b/rainbow.txt @@ -3,4 +3,5 @@ orange yellow green blue -purple +indigo +violet How to read a diff Here are the more meaningful lines in the example below:
The first line shows which two files are being compared. Is normally the same file but can be set up to be different. Original is going to be A and new is B diff --git a/rainbow.txt b/rainbow.txt - will be used for representing changes in file a, and + for changes in file b --- a/rainbow.txt +++ b/rainbow.txt Next the chunks will start. A diff will only show portions or chunks of files that were modified together with some context (lines before and after). The first thing is the header. In this case it means: From file a, 4 lines are extracted starting from line 3 (in the chunk) and from file b, 5 lines are extracted starting from line 3 (in the chunk) @@ -3,4 +3,5 @@ orange Finally, the changes are shown with the symbols relating to the files as explained above yellow green blue -purple +indigo +violet In this case, in file A, the last line was purple. Meanwhile in file B, purple was removed and indigo and violet were added
How to create a diff There a different ways to run the git diff command according to what you want to compare:
git diff will show changes in the working directory that are not in staging yet. In other words, compares the working directory with the staging area
git diff HEAD will compare HEAD and the working directory. In other words, it shows changes in the working directory since the last commit. Different to above it will show staged and unstaged changes.
git diff --staged or git diff --cached will compare HEAD and the staging area. It will show the changes that will be committed.
It is also possible to combine this options with different targets:
git diff \u003cOPS\u003e file shows changes for a specific file
git diff \u003cOPS\u003e branch1..branch2 or git diff \u003cOPS\u003e branch1 branch2 will compare two branches. The order in which they are passed determines which is considered A and B (First is always the “base”). In addition, adding a file name at the end, will limit the comparison to a specific file
git diff commit1..commit2 will compare changes between two commits, passing the hash of the commits. Same as above, the order influences which is A and B.
`,description:"",tags:null,title:"Diffs",uri:"/git/diffs/"},{breadcrumb:"Notes \u003e Git",content:`What are branches? In GIT each commit, identified by a unique hash, has a reference to its parent commit
Branches can be tought of as alernate timelines in a project. They exist at the same time but are completely different contexts, so changes in a branch dont affect other branches.
In detail, branching happens when a commit has more that one child commit. In a techinical manner, branches are a reference to a commit (that has a series of parent commits) like shown below:
HEAD pointer Points to the current location in the repository ( a branch reference ) The branch reference is just the reference to the last commit Commands for branching git branch Shows all local branches of a repository. git branch -r will show remote branches and git branch -a will show all of them. If used with a name (git branch name) it will create (but not switch) a new branch based on the current HEAD. With the -d flag, it can be used to delete branches. (-D to force delete) The git branch -m new_name will change the name of the current branch (current HEAD) git switch Allows to switch between existing branches git switch -c name can be used to create a new branch and switch to it git checkout Allows to switch between existing branches Older command with lots of extra functionalities git checkout -b name can be used to create a new branch and switch to it Attention Uncommited changes will be lost if switching branches (can be stashed). Only if the changes do not conflict, for example new files that dont exist, will these come to the new branch
Merging branches When working with branches, most of the use cases involves bringing the work done in a branch into the original (or other) branch.
Incorporating changes from one branch to the other, is done with the git merge command.
git merge always merges to the current HEAD branch.
In summary the process is the following:
Switch to target branch (e.g git switch master) Merge branch (e.g git merge mybranch) According to the type of differences accross the branches, there could be:
Different types of merges Merge conflicts Merge Types Fast forward:
Simplest type of merge Target branch is just behind merging branch In other words, the merging branch just have some extra commits. Always automatically merged Merge Commit:
More common There is commits in the target branch that are not in the merging one. Can be automatically merged or not, depending on conflicts. Makes a commit. Normally asks for a commit message Merge Conflicts If a merge operation involves a file that is present but different in both branches, a merge conflict occur. In that case, git cannot decide which “version” should stay, so automatic merging is deactivated and one has to resolve the conflicts manually:
Files with conflict get “decorated” with \u003c\u003c\u003c\u003c\u003c\u003c\u003c HEAD and \u003e\u003e\u003e\u003e\u003e\u003e\u003e branch to indicate which changes belong to which branch One decides which to keep (or combines both) One removes the decorators The files must be saved and committed to apply the merge Tools like VSCode offer a GUI for resolving conflicts `,description:"",tags:null,title:"Branches",uri:"/git/branches/"},{breadcrumb:"Notes",content:"",description:"",tags:null,title:"Azure",uri:"/azure/"},{breadcrumb:"Notes \u003e Azure",content:`Definition Delivery of computing services over the internet (common IT infrastructure such as virtual machines, storage, databases, and networking). Cloud services also expand the traditional IT offerings to include things like Internet of Things (IoT), machine learning (ML), and artificial intelligence (AI). It allows to move capital expenditure (servers, datacenters, etc) into operational expenditure (cloud costs). Benefits High availability: Always available (SLA) Scalability: Vertical: Ease to add more power to machines Horizontal: Ease to add more machines Reliability: Decentralized, globally distributed Predictability: Constant performance and costs Security: Different levels of access, built in protection Governance: Ease to implement compliance Manageability: Ease to add/delete resources and interact with infrastructure Models 3 different models:
Public Cloud: Build controlled and maintained by a third party Available to the public Hybrid Cloud: Use private and public clouds interconnected Private Cloud: Cloud used by a single entity On premises Offers total control More cost and complexity Service Types: Cloud providers distinguish 3 types of service. Each service place responsibility to the provider or the customer side (Shared responsibility model). These are:
IaaS: Infrastructure as a service Most responsibility to the customer Provider maintains and manages physical infrastructure Examples include VM services, virtual networks, etc. PaaS Platform as a service Middleground Provider additionally maintains OS, dev tools, etc. Examples like Azure app services, AKS SaaS Software as a service Complete managed solution Examples include Managed dbs The different responsibilities for each service type are shown below
`,description:"",tags:null,title:"Cloud Concepts",uri:"/azure/cloudconcepts/"},{breadcrumb:"Notes \u003e Git",content:`The key component of git is the commits. A commit is single point in the Git history; the entire history of a project is represented as a set of interrelated commits.
Good practice By good practice, commits should be atomic. This means each commit should relate to only one feature. This makes it easier to control, and roll back.
Commit Message A commit contains the current contents of the index and the given log message describing the changes.
The commit message should follow:
Is recommended by Git that the message is written in present tense + imperative Similar to giving orders to the machine Some examples: Fix bug in template code Make script compatible with centos Ammending Ammending a commit replaces the tip of the current branch by creating a new commit that starts from the last one.
In practice it can be used to “fix” the last commit.
Remember The changes to ammend need to be staged
Ignoring files and directories Files that should not bre tracked by git can be specified in a .gitignore file.
The file supports files, directories and also glob expressions to match several files at the same time.
More information can be seen in the https://git-scm.com/docs/gitignore
`,description:"",tags:null,title:"Commiting",uri:"/git/commiting/"},{breadcrumb:"Notes \u003e Git",content:`Below are some of the most used commands when using git from the command line:
git init Initializes a git repository in the current directory. This is done by creating a hidden .git directory. Git docs If used with a name as an argument like git init exampleIt will first create a folder called example and inside this folder initialize the repo.
Tip To make a directory not a git repository anymore, just delete the hidden .git folder.
git status Shows the current state of the files according to the basic workflow (un-tracked, staged). Git docs git add Normally used like git add file it adds a file to staging. This is done before committing because only staged files (or changes) are added to the commit.
Git docs
If used like
git add -Aor
git add .Adds all the files that have been modified
git commit Allows to create a commit for the staged changes. Git docs To commit with a message directly:
git commit -m "message"Default editor If called without arguments, the command will prompt for a commit message using the default editor. This can be changed with git config --global core.editor More info here git log Retrieves a complete list of commit history for the repo
Its output can be compacted with the --oneline flag Git docs `,description:"",tags:null,title:"Basic Commands",uri:"/git/basic-commands/"},{breadcrumb:"Notes",content:"",description:"",tags:null,title:"Categories",uri:"/categories/"},{breadcrumb:"Notes \u003e Kubernetes",content:`Definition The networking model in Kubernetes is defined as follows:
Containers inside a pod have their own network namespace and can reference each other using localhost Each node in the cluster has an assigned address pool (e.g 10.244.0.0/24), and pods within the node get assigned an IP from this pool All pods in the cluster are able to communicate with each other without NAT All pods can communicate with the nodes they are placed in or other nodes without NAT Implementation Kubernetes defines how the networking should work, but does not provide an actual implementation. For this, different CNI Plugins can be used.
The CNI model improves modularity in the Kubernetes ecosystem. Different plugins offer unique combinations of features to accommodate a wide range of use cases and environments. Any CNI plugin will provide the standard set of Kubernetes networking features but can also expand on them, such as by integrating with other network technologies and services.
Popular ones include:
Calico Flannel Cilium Weave Net `,description:"",tags:null,title:"Networking",uri:"/k8s/networking/"},{breadcrumb:"Notes",content:"",description:"",tags:null,title:"Tags",uri:"/tags/"},{breadcrumb:"Notes \u003e Linux \u003e Using a Linux System \u003e Shell",content:`Tmux is an application that allows the creation of different terminals inside a single shell (terminal multiplexer). It provides several benefits like:
Persistent SSH sessions: Specially practical when connecting to remote servers, tmux sessions will remain even if ssh connection is lost, as long as the machine is not turned off Terminal management: Sessions can have several windows (tabs) and each window supports several panes. THese split terminals can display information simultaneously and be “grouped” to execute the same command at the same time. Allow programs running on a remote server to be accessed from multiple different local computers. Basic usage Below are basic tmux actions:
tmux # Create new unnamed session tmux new # Same tmux new -s "name" #Create a new session named "name" tmux ls #List sessions tmux attach -t "name" #Attach to running session "name"tmux control mode When invoking tmux, a new nameless session is created and a terminal is displayed. However any key typed will be thought of as part of the command for the terminal, not for tmux. For this, to control tmux one has to enter in control mode. Then tmux will be “listening” to keys for configurations.
In default installations the way to enter control mode is to type the “prefix key” which is Ctrl + b
Attention All commands shown below assume the default configuration but they can be customized
Panes Every terminal inside tmux belongs to one pane, this is a rectangular area which shows the content of the terminal inside tmux. Because each terminal inside tmux is shown in only one pane, the term pane can be used to mean all of the pane, the terminal and the program running inside it.
Each pane appears in one window. A window is made up of one or more panes which together cover its entire area. This means the windows can be “carved” into different panes:
Create new pane A new pane is always created in reference of the current pane
Vertical split: PREFIX + % Horizontal split: PREFIX + "
Move between panes Left: PREFIX + ← Right: PREFIX + → Up: PREFIX + ↑ Down: PREFIX + ↓
Close active pane PREFIX + x
Windows A window (or tab) is made up of one or more panes which together cover its entire area. Every window has a name - by default tmux will choose one but it can be changed by the user
Multiple windows are grouped together into sessions. Each window in a session has a number, called the window index. Even if the window is deleted, the index does not move.
Each session has one current window, this is the window displayed when the session is attached and is the default window for any commands that target the session. If the current window is changed, the previous current window becomes known as the last window.
Create new window Either tmux can be called inside of tmux
tmux new-window tmux new-window -n "name"Or the keyboard can be used:
PREFIX + c
Rename window Either
tmux rename-window nameor
PREFIX + , and then type the name
Move between windows Go to next window (on the right): PREFIX + n Go to previous window (left): PREFIX + p
Close window PREFIX + \u0026
Sessions Sessions group one or more windows together. They are the “bigger” block of tmux and allow organization between work processes. One can attach to different sessions.
They are always named, but when no name is given in creation, tmux assigns one (normally a number)
Create a new session tmux can be called from outside tmux to create a new session
tmux tmux new tmux new -s nameList sessions tmux lsAttach/dettach to a session tmux attach -t nameTo detach:
PREFIX + d
Rename active session Either
tmux rename-session nameor
PREFIX + $
Switch sessions PREFIX + s to go to an interactive selection menu
Close session To close the session, all windows must be closed. This can be done by typing exit in each pane, or by the commands seen above
Customization tmux is highly customizable. By defining a ~/.tmux.conf file, one can set up different commands, key combinations and other behaviors. This file is read by tmux at initialization, and a re-read can be forced with tmux source-file file
Tip ALl customizations can also be done “non-persistent” by typing PREFIX + : and then entering the command
Example customization The example below is inspired from Learn Linux TV shows pretty cool options. For complete documentation, here is the config reference
# Change prefix set-option -g prefix C-j # Ctrl + j set-option -g prefix2 C-f # Second prefix optional (Ctrl+f) # Create new option (Prefix + r now reloads the config) bind-key r sourcefile ~/.tmux.conf \\; display-message "tmux.conf reloaded" # Integrate mouse for changing terminals, adjusting sizes, etc. set -g mouse on # Use Alt-arrow keys without prefix key to switch panes (No prefix) bind -n M-Left select-pane -L bind -n M-Right select-pane -R bind -n M-Up select-pane -U bind -n M-Down select-pane -D # Set easier window split keys bind-key v split-window -h # Ctrl + v creates a vertical split bind-key h split-window -v # Ctrl + h creates a horizontal split # Shift arrow to switch windows bind -n S-Left previous-window bind -n S-Right next-window # Synchronize panes (same command) with prefix+y (toggle) bind-key y set-window-option synchronize-panes\\; display-message "synchronize mode toggled."`,description:"",tags:null,title:"Tmux",uri:"/linux/using-a-linux-system/shell/tmux/"}]