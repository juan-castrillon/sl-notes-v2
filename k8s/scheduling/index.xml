<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scheduling on SL Notebook</title><link>https://slnotes.blog.castrillon.ch/k8s/scheduling/index.html</link><description>Recent content in Scheduling on SL Notebook</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Sun, 11 May 2025 18:03:14 +0200</lastBuildDate><atom:link href="https://slnotes.blog.castrillon.ch/k8s/scheduling/index.xml" rel="self" type="application/rss+xml"/><item><title>Manual Scheduling</title><link>https://slnotes.blog.castrillon.ch/k8s/scheduling/manual/index.html</link><pubDate>Mon, 21 Apr 2025 15:44:48 +0200</pubDate><guid>https://slnotes.blog.castrillon.ch/k8s/scheduling/manual/index.html</guid><description>As part of the scheduling process, the kube-scheduler updates the nodeName field on pods. This behavior can be emulated manually to &amp;ldquo;skip&amp;rdquo; the scheduler and manually assign a pod to a node
E.g The pod below will always go to node1
apiVersion: v1 kind: Pod metadata: name: nginx spec: nodeName: node1 containers: - name: nginx image: nginx Updating running pods Once a pod has been assigned a node, the nodeName field cannot be edited.</description></item><item><title>Priority</title><link>https://slnotes.blog.castrillon.ch/k8s/scheduling/prio/index.html</link><pubDate>Sun, 11 May 2025 18:03:14 +0200</pubDate><guid>https://slnotes.blog.castrillon.ch/k8s/scheduling/prio/index.html</guid><description>As shown in process, part of the scheduling depends on the defined priority of the pods to be scheduled. To control this, k8s allows for the definition of objects called PriorityClass which determined how critical a pod is.
The range of priorities that can be defined, can go from 10^9 to -2^31 (-2,147,483,648). Priority classes are also used for system components deployed (thing of the kube-system namespace), and these ones go from 10^9 to 2*10^9 (and maybe more).</description></item><item><title>Scheduler Process</title><link>https://slnotes.blog.castrillon.ch/k8s/scheduling/process/index.html</link><pubDate>Thu, 24 Apr 2025 12:28:27 +0200</pubDate><guid>https://slnotes.blog.castrillon.ch/k8s/scheduling/process/index.html</guid><description>The default kube-scheduler works in the following way:
When a pod is submitted to the scheduler, it enters a scheduling queue along with other pending pods. Filter Phase: Nodes that cannot meet the pod&amp;rsquo;s resource requirements (e.g., nodes lacking 10 CPUs) are filtered out. Scoring Phase: Remaining nodes are scored based on resource availability after reserving the required CPU. For example, a node with 6 CPUs left scores higher than one with only 2.</description></item><item><title>Multiple Scheduler and Profiles</title><link>https://slnotes.blog.castrillon.ch/k8s/scheduling/multiple/index.html</link><pubDate>Thu, 24 Apr 2025 12:08:05 +0200</pubDate><guid>https://slnotes.blog.castrillon.ch/k8s/scheduling/multiple/index.html</guid><description>Multiple schedulers If a different scheduling algorithm, or more particular requirements are needed, a custom scheduler can be deployed. Moreover, once can even run multiple schedulers simultaneously alongside the default scheduler and instruct Kubernetes what scheduler to use for each of the pods.
Following good practices, the scheduler can be packaged and distributed as a binary (to create a service) or a container (to create a K8s deployment).
Once deployed, pods can choose which scheduler to use, for example:</description></item><item><title>Static Pods</title><link>https://slnotes.blog.castrillon.ch/k8s/scheduling/static/index.html</link><pubDate>Thu, 24 Apr 2025 11:15:21 +0200</pubDate><guid>https://slnotes.blog.castrillon.ch/k8s/scheduling/static/index.html</guid><description>K8s provides a way to &amp;ldquo;override&amp;rdquo; the scheduler and deploy pods statically. These static pods are managed directly by the kubelet daemon on a specific node, without the API server observing them.
The kubelet automatically tries to create a mirror Pod on the Kubernetes API server for each static Pod. This means that the Pods running on a node are visible on the API server, but cannot be controlled from there.</description></item><item><title>Resource Limits</title><link>https://slnotes.blog.castrillon.ch/k8s/scheduling/resources/index.html</link><pubDate>Thu, 24 Apr 2025 10:25:14 +0200</pubDate><guid>https://slnotes.blog.castrillon.ch/k8s/scheduling/resources/index.html</guid><description>When scheduling, K8s takes into consideration available resources (CPU and Memory) at the nodes. However, by default, there is no upper limit, so eventually pods could &amp;ldquo;fill up&amp;rdquo; nodes. To prevent this, and to ensure certain resources at the container, pod or namespace level, the concept of request and limits is introduced.
Pod (Container) level In each container in a pod, two thing can be defined to specify resource requirements and limits:</description></item><item><title>Node Selectors and Affinity</title><link>https://slnotes.blog.castrillon.ch/k8s/scheduling/selectors/index.html</link><pubDate>Mon, 21 Apr 2025 15:45:29 +0200</pubDate><guid>https://slnotes.blog.castrillon.ch/k8s/scheduling/selectors/index.html</guid><description>As opposed to taints which prevents pods from going to nodes, node labels are used either with selector or affinity definitions to make a pod run in a certain node.
Node Selectors In a simple scenario, node selector definitions (part of the pods YAML) are used to match labels in the nodes. The example below binds the pod with nodes that have the size=Large label
apiVersion: v1 kind: Pod metadata: name: myapp-pod spec: containers: - name: data-processor image: data-processor nodeSelector: size: Large Node Affinity For more complex scenarios, node affinity definitions are used.</description></item><item><title>Taints and Tolerations</title><link>https://slnotes.blog.castrillon.ch/k8s/scheduling/taints/index.html</link><pubDate>Mon, 21 Apr 2025 15:45:21 +0200</pubDate><guid>https://slnotes.blog.castrillon.ch/k8s/scheduling/taints/index.html</guid><description>When scheduling, the concept of taints and tolerations solves the following problem:
How to make nodes only run certain pods?
Attention It&amp;rsquo;s important to note that taints and tolerations does NOT make pods run in certain nodes. It is a limitation on nodes.
Taints Taints are conditions applied to nodes that describe which pods to reject. They have 3 main components:
Key: Label key that the toleration needs to match Value: Label value that the toleration needs to match Behavior: This tells the scheduler what exactly to do: NoSchedule: If a pod cannot tolerate the taint, it will not be scheduled in the node.</description></item></channel></rss>